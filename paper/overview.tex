%!TEX root = paper.tex

\section{Overview}
\label{sec:examplea}

We present an overview of our approach by considering an example.  To
highlight the ideas, we abstract over some technical details, which
are fully formalized in the rest of the paper.  The example itself is
also implemented in our implementation.

\paragraph{Computing order statistics.}

\begin{abstrsyn}

\input{figures/quickselect-aligned}

Suppose that we wish to compute a series of order statistics queries
on a list \texttt{l}. To this end, we can use
quickselect~\cite{Hoare:1961}, which give a list \texttt{l} and a rank
\texttt{k}, returns the element of \texttt{l} with rank \texttt{k}.
As implemented in an ML-like language in \ref{fig:qs-unstaged},
function \texttt{qs} partitions the list by using the first element as
a pivot and then recurs on one of the two resulting sides to find the
desired element based on the relationship of \texttt{k} to the size of
the first half \texttt{i}.  Assuming that the input is uniformly
randomly ordered (which can be achieved by pre-permutation of the
input), function \texttt{qs} runs in expected linear time.
%
\footnote{We assume that the list is prepermuted to guarantee the
  expected linear time behavior.} 
%
Using function \texttt{qs}, we can perform many order statistics
queries, for example but with $m$ different ranks
$\mathtt{k_1},\dots,\mathtt{k_m}$ as follows
%
\begin{lstlisting}
qs l @$\mathtt{k_1}$@; qs l @$\mathtt{k_2}$@; @$\ldots$@; qs l @$\mathtt{k_m}$@.
\end{lstlisting}
%
Unfortunately, this approach requires $O(n \cdot m)$ time, making it
expensive to perform order statistics on more than a small number of
times. This is unfortunate, because for example, because we wish to be
able to map \texttt{qs} to over another list and do so
efficiently. 

One way to regain efficiency is to change the algorithm that we use to
perform the computation in multiple passes.  One option would be to
pre-sort the input list \texttt{l} into a sorted list \texttt{s} in
the first pass, and then perform lookups in \texttt{s} in the second
pass.  Unfortunately, since \texttt{s} is a list, a lookup would
require linear time (also on average), leading to no improvement in
efficiency.  We can resolve this issue by using a data structure that
can provide faster access.  For example, after sorting \texttt{s} we
can copy in into an array and then perform binary searches to find the
element with the desired rank (copying the input to an array and
sorting would not work well in a functional language).  As another
option, we can represent construct a binary search tree from the input
and perform size-based searches on the binary search tree to find the
elements with the desired rank.  Such an approach would fit nicely
into the functional paradigm of computing but requires additional
algorithmic ingenuity: to ensure efficiency we would need to store at
each note the size of its subtree (these are sometimes called
``augmented trees'').  In summary, we are able to improve efficiency
by replacing our simple solution with a multi-pass algorithm that
first preprocesses the input to generate a lookup data structure and
then in the second pass performs fast lookups.  As outlined above, in
addition, this transformation is highly non-trivial as it involves
reasoning about intricate algorithmic concerns, and implementing more
complex algorithms.

Such transformations, called pass-separation by J{\o}rring and
Scherlis are commonly employed.  For example, as briefly mentioned in
the introduction, modern graphics software is written exactly in this
way.  Unfortunately, as the example illustrates, they lead to complex
software, leading to the question of whether it would be possible to
perform such transformations automatically.  We answer this question
affirmatively.  Our approach is to start with a staged language and
then use a splitting algorithm to generate multi-pass programs from
staged programs.


\subsection{Staging}

The idea behind staged programming is to use staging annotations or
types to indicate the stage of each expression.  To apply this
technique to our example, suppose that we have a language with two
worlds representing the two stages and a third, {\em ground} world
that contains terms with no staging.

We can write a staged version of \texttt{qs}, called
\texttt{qss} in \ref{fig:qs-staged}, by making the input list a
stage-1 and the rank a stage-2 argument. 
%
More precisely the input list
has type $\curr\mathrm{list}$ (an integer list ``now''), the rank has
type $\fut\rmint$ (an integer in the ``future''), and return has type
$\fut\rmint$.  
%
We can then obtain \texttt{qss} by annotating the body of \texttt{qs}
with the staging annotations \texttt{prev} and \texttt{next}, which
transition between a stage-1 and stage-2 code, and $\texttt{g}$, which
marks ground terms.  
%
We also use a third function
\texttt{hold}, which can be implemented with \texttt{prev} and
\texttt{next}, for promoting a stage-1 integer to a stage-2 integer;
the signature of \texttt{hold} is
% 
\lstinline{1`hold : ^int -> $`2`int`}.

To help readability, we write the first-stage
computations in red, and the second-stage computations in blue.

Our type system ensures that the staging annotations are consistent,
in the sense that computations marked as first-stage cannot depend on
ones marked as second-stage.
%
The process of adding staging annotations to unstaged code has been
the subject of extensive research (\ref{sec:related}). We assume that
these annotations have been provided; we do not consider the problem
of generating them. In this example, there are other ways to annotate
\texttt{qs}, but we chose annotations that maximize the work performed
in the first stage


% The $\curr$ and \texttt{grnd} (``ground'') annotations
% further distinguish those first-stage computations which do not
% contain any embedded second-stage code. (Code \emph{inside}
% \texttt{grnd}, like the \texttt{part}ition function, is guaranteed not
% to contain second-stage code; first stage code outside a
% \texttt{grnd}, like \texttt{qsStaged}, may.) We will discuss the
% importance of this additional distinction in \ref{sec:splitting}.


% Ignoring for the moment all \texttt{grnd} annotations, the first-stage code in
% \texttt{qsStaged} looks like \texttt{qSelect}: in the first stage, terms of
% non-circle type are available for immediate use. Indeed, we case on the list as
% usual, and in the \texttt{Cons} branch, \texttt{part}ition it.



\subsection{Splitting Staged Programs}

\input{figures/quickselect-split-aligned}

The staged quickselect code shown in \ref{fig:qs-staged} makes
explicit the staging of all terms, making it natural to ask, whether
we can transform this code into manually implemented multi-pass code
described above and match the efficiency improvements of the manual
implementation. Considering the algorithmic knowledge and the
considerations needed, this may seem like a tall order.
Interestingly, our splitting algorithm achieves exactly this.

In the rest of this section, we present a brief, high-level overview
of the main ideas behind this algorithm using the quickselect example.
%
When applied to the staged code in \ref{fig:qs-staged}, our splitting
algorithm yields a two-pass program that uses the binary-search-tree
based implementation.  Specifically, in its first pass, the two-pass
program takes the input list and constructs a probabilistically
balanced binary search tree, which is isomorphic to a treap data
structure~\cite{treaps}.  In the second pass, the program performs,
for each rank, a binary search tree lookup, by walking the tree to
find the element with the desired rank.
%
\ref{fig:qs-staged} illustrates the code for the first and second
passes \texttt{qs1} and \texttt{qs2} of the staged function
\texttt{qss}.


To create the multi-pass algorithm, the splitting algorithm operates
by composing local transformations on the subterms of the input
program.  In particular, the algorithm has no special knowledge of the
quickselect algorithm, binary search trees, or how to perform lookups
on binary search trees, but it is able to derive all of these from the
input program.
%

The splitting algorithm scans the program code for first-stage
computations (which depend only on first-stage values) and separates
them into the function, \texttt{qs1} of the first pass. This function
performs the stage-1 computations and places the results into a
boundary data structure that both records the control flow and the
results from the first stage computation at each control-flow point.
The function \texttt{qs1} performs all the recursive calls and
evaluate all instances of the partition function, which depend only on
the input list.  The function produces a boundary data structure that
collects the results from all recursive calls along with a tag that
indicates the control branches taken.  Since \texttt{qss} has a binary
control structure (casing on the list), the boundary is a binary tree.
To ensure that the trace contains the necessary results to complete
the execution in the second pass, where second-stage values may be
used, the splitting algorithm includes in the nodes of the tree
information such as the "pivot" used for splitting and the size of the
subtree at that note.  The resulting tree is thus not just a tree but
a binary search tree, keyed by the pivot and augmented with "size"
information.

As the splitting algorithm scans \texttt{qss} for computations that
can be performed in the first stage, it also collects computations
that must be left to the second stage in a separate function
\texttt{qs2}.  This function, which is executed in the second pass,
takes as argument the boundary and the second-stage argument (the
rank) and performs a lookup in the boundary data structure.  But how
does \texttt{qsStage2} knows to traverse the tree? It does not. Since
the splitting algorithm recorded the control flow of the first stage
execution in the boundary data structure, \texttt{qs2} simply follows
this control structure and performs at each point the parts of the
computation from \texttt{qss} that can now be performed in the
presence of the second-stage argument (the rank).  In the context of
our example, this performs a lookup on the boundary data structure by
using the supplied rank.

\begin{theorem}
  Consider an execution of \texttt{qs1} with a randomly permuted input
  of $n$ keys and performing $m$ executions of \texttt{qs2} with the
  result of \texttt{qs1}.  The total run-time for this computation is
  $\Theta(n\log{n} + m\log{n})$ in expectation.
\end{theorem}


\begin{comment}

An astute programmer, having noticed that \texttt{quickselect} can be staged in
this fashion, might try to split it into a pair of functions, one which performs
all the work depending only on \texttt{l} (the first stage), and one which uses
that partial result and \texttt{k} to compute the element with
rank \texttt{k} in \texttt{l}. 

Intuitively, \texttt{l} determines the result of all calls to
\texttt{part}ition, and \texttt{k} only determines which calls are made. So we
can preprocess \texttt{l} by recursively dividing it into halves smaller and
greater than the pivot---that is, building a binary search tree. Then, once we
have \texttt{k}, we can recur on this tree, choosing whichever branch has the
\texttt{k}${}^\textit{th}$ leftmost element until we reach a leaf. And because
\texttt{part}ition contains no second-stage code, we can run it entirely in the
first stage.

We have implemented this splitting of quickselect in \ref{fig:qs-split}.
\texttt{qSelect1} builds a binary search tree from the list \texttt{l}, and
\texttt{qSelect2} takes such a tree and a rank \texttt{k} and computes the
answer. This allows us to efficiently perform many order statistics queries on
\texttt{l} by caching the tree and reusing it for many different ranks
$\mathtt{k_1},\dots,\mathtt{k_m}$:
%
\begin{lstlisting}
let b = qSelect1 l in
  qSelect2 b @$k_1$@
  qSelect2 b @$k_2$@
   @$\vdots$@ 
  qSelect2 b @$k_m$@.
\end{lstlisting}

Assuming \texttt{l} contains $n$ elements, this optimization changes the
asymptotic complexity from expected (randomized) $\Theta(n \cdot m)$ to
$\Theta(n\log{n} + m\log{n})$, which for any $m \approx n$ reduces the
complexity from $\Theta (n^2)$ to $\Theta(n\log{n})$---a significant improvement. 

In this paper, we develop a splitting algorithm
(\ref{sec:splitting,sec:implementation}) which, given a program $e$ in \lang,
produces an equivalent pair of programs which correspond precisely to the two
stages of computation in $e$.
(Splitting is always possible because the staging annotations in $e$ are
consistent, because $e$ is well-typed in \lang.) In the case of
\texttt{qsStaged}, our splitting algorithm produces the algorithm described
above.
\end{comment}
%\texttt{qSelect l k = qSelect2 (qSelect1 l) k}.

%Because the tree passes information across the stage boundary, we call it the
%\emph{boundary data structure}.

%Note that the desired optimized code shows above is intellectually more
%sophisticated than the code that we have started with: the optimized code is
%able to create a data structure, a balanced binary tree augmented with indexing
%information, and use a binary search technique over this tree to compute the
%result asymptotically more efficiently.

%In fact, based our teaching experience, we can imagine this kind of problem to
%be a moderately difficult exam question in an undergraduate algorithms class, as
%it not only requires understanding of data structures such as binary search
%trees but also requires modifying them to augment with indexing information to
%support rank-based search.

%(I also want to make it clear that recognizing \lang's appropriateness for this
%is itself is a contribution.)

\subsection{Quickselect in more detail}

Carlo writes this.



\end{abstrsyn}
