%!TEX root = paper.tex

\section{Overview}
\label{sec:overview}

\begin{abstrsyn}

Suppose that we wish to perform a series of order statistics queries
on a list \texttt{l}. To this end, we can use the quickselect
algorithm~\cite{Hoare:1961}, which given a list \texttt{l} and a rank
\texttt{k}, returns the element of \texttt{l} with rank \texttt{k}.
As implemented in an ML-like language in \ref{fig:qs-unstaged},
\texttt{qs} partitions \texttt{l} using the first element as
a pivot and then recurs on one of the two resulting sides, depending on
the relationship of \texttt{k} to the size \texttt{i} of the first half, in
order to find the desired element.  
If the index is out of range, a default value of \texttt{0} is returned.
Assuming that the input is uniformly
randomly ordered (which can be achieved by pre-permuting the input), \texttt{qs}
runs in expected $O(n)$ time.
%
Using \texttt{qs}, we can perform $m$ different order statistics queries with
ranks $\mathtt{k1},\dots,\mathtt{km}$ as follows:
%
\begin{lstlisting}
(qs l k1, qs l k2, ..., qs l km)
\end{lstlisting}
%
Unfortunately, this approach requires $O(n \cdot m)$ time.

We can attempt to improve on this algorithm by factoring out computations shared
between these calls to \texttt{qs}. In particular, the comparisons performed by
\texttt{partition} depend only on \texttt{l}, although different calls to
\texttt{partition} are made by each call to \texttt{qs}.

A first cut at a solution might be to first sort \texttt{l} into
\texttt{s} and then perform lookups in \texttt{s}.
Unfortunately, lookups in \texttt{s} are also $O(n)$ on average, since
\texttt{s} is a list, leading to no improvement in efficiency.  

A more sophisticated solution is to construct a binary search
tree out of \texttt{l} and then simply look for the \texttt{k}th
leftmost element of that tree. To do this efficiently requires one more
innovation---augmenting the tree by storing at each node the size of its left
subtree. This approach has (expected) runtime $\Theta(n\log{n} + m\log{n})$, a
nearly linear-time improvement in $m$.

In both cases, we replace our simple solution with a two-part algorithm which
first processes the input into a lookup data structure, and then subsequently
performs fast lookups. Improving algorithms in this way is highly non-trivial,
as it requires reasoning about intricate algorithmic concerns, and implementing
more complex algorithms and data structures.  

In this paper, we present a splitting algorithm which automatically transforms
staged programs into multi-part programs. In certain cases, this algorithm even
improves input algorithms in the way described above!

\subsection{Staging}

%The example itself is also implemented in our implementation.

The idea behind staged programming is to use staging annotations---in our case,
guided by types---to indicate the stage of each subterm. 

In \ref{fig:qs-staged} we show a staged version of \texttt{qs}, called
\texttt{qss}, where first-stage code is colored red, and second-stage blue. In
\texttt{qss}, we regard the input list \texttt{l} as arriving in the first stage
(with type $\curr\mathrm{list}$, a list ``now''), the input rank \texttt{k} as
arriving in the second stage (with type $\fut\rmint$, an integer in the
``future''), and the result as being produced in the second stage (with type
$\fut\rmint$).

\texttt{qss} is obtained from \texttt{qs} by wrapping certain computations with
\texttt{prev} and \texttt{next}, signaling transitions between first- and
second-stage code, and $\texttt{gr}$, signaling ground terms.  
%
We also use a function
\lstinline{1`hold : ^int -> $`2`int`},
to promote first-stage integers to second-stage integers.
%
Our type system ensures that the staging annotations in \texttt{qss} are
consistent, in the sense that computations marked as first-stage cannot depend
on ones marked as second-stage.

The process of automatically adding staging annotations to unstaged code,
called \emph{binding time analysis}, has been
the subject of extensive research (\ref{sec:related}). In this paper, we do not
consider this problem, instead assuming that the annotations already exist.
In the case of \texttt{qss}, we have specifically chosen annotations which
maximize the work performed in the first stage.

\subsection{Splitting Staged Programs}

\input{figures/quickselect-split}

In the rest of this section, we present a high-level overview of the main ideas
behind our splitting algorithm, applied to \texttt{qss}.
%
Splitting \texttt{qss} yields a two-part program that creates a
probabilistically balanced, augmented binary search tree (which is isomorphic to
a treap data structure~\cite{SeidelAr96}) as an intermediate data structure. In
particular, its first part (\texttt{qs1}) constructs such a binary search tree
and its second part (\texttt{qs2}) traverses the tree, using the embedded size
information to find the element of the desired rank.
%
The code for \texttt{qs1} and \texttt{qs2} is in \ref{fig:qs-split}.

%Our splitting algorithm is guided by the local structure of the input program.
%In particular, the algorithm has no special knowledge of the quickselect
%algorithm, binary search trees, or how to perform lookups on binary search
%trees, but it is able to derive them from the input program.

Our splitting algorithm scans \texttt{qss} for first-stage computations,
gathering them into \texttt{qs1}. Given \texttt{l}, this function performs these
computations and places the information needed by the subsequent function into a
boundary data structure.
%
In particular, \texttt{qs1} performs all recursive calls and evaluates all
instances of \texttt{partition} (since it depends only on \texttt{l}). It
produces a boundary data structure that collects the results from these
recursive calls, tagged by the branch (\texttt{LT}, \texttt{EQ}, or \texttt{GT})
in which that call occurred. Since the recursive calls occur in two different
branches (\texttt{LT} and \texttt{GT}) the boundary structure is a binary tree.  
%
Lastly, it records \texttt{i} (the size of the left subtree) and \texttt{\#1 ht}
(the pivot/head of the list) in the boundary structure, because those
computations are \emph{held} for use in the second stage.
%
The final result is a binary search tree augmented with size information, and
whose keys are the pivots.

Our splitting algorithm simultaneously scans \texttt{qss} for second-stage
computations, gathering them into \texttt{qs2}. This function is given the boundary
data structure and the rank \texttt{k}, and it finishes the computation. Now that
\texttt{k} is known, the conditional on \texttt{compare k i} can be evaluated,
choosing which recursive call of \texttt{qss} is actually relevant for this
\texttt{k}. Since the boundary data structure contains the first-stage data for
\emph{all} of the recursive calls, performing these comparisons essentially
walks the tree, using the rank along with the size data \texttt{i} in order to
look up the \texttt{k}th leftmost node in the tree.

\begin{theorem*}
  Consider an execution of \texttt{qs1} with a randomly permuted input
  of $n$ keys and performing $m$ executions of \texttt{qs2} with the
  result of \texttt{qs1}.  The total run-time for this computation is
  $\Theta(n\log{n} + m\log{n})$ in expectation.
\end{theorem*}
\begin{proof}
  We note first that \texttt{qs1} is
  isomorphic to a quicksort algorithm but builds a binary search tree
  instead of combining the returned results in a list.  The run-time
  for \texttt{qs1} is thus (expected) $\Theta(n\log{n})$.
%
  The depth of the tree generated by \texttt{qs1} is isomorphic to the
    depth of a run of the quickselect algorithm, or equivalently to the
    span of a parallel implementation of the quicksort algorithm, both
    of which are known to be (expected) $\Theta(\log{n})$.
%
    By inspection of \texttt{qs2}, we can see that the algorithm takes
    time proportional to the depth of the binary search tree, which is
    $\Theta(\log{n})$. Consider now a random variable for each of the
    $m$ invocations of \texttt{qs2}, each of which takes expected
    $\Theta(\log{n})$ time.  Since the total time is the sum of these
    random variables, and the expectation of the sum is the sum of
    the expectations of the random variables, the bound follows.
\end{proof}

\end{abstrsyn}
