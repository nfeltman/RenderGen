\section{Introduction}

Consider a function $F$ which takes an $x$ and $y_1,\dots,y_{m-1}$, and computes
$f(x,y_i)$ for each $y_i$:
\begin{lstlisting}
fun @$F(x, y_0, y_1, \ldots, y_{m-1})$@ = 
  @$f(x, y_0);  f(x, y_1); \ldots; f(x, y_{m-1})$@
\end{lstlisting}
%
This scenario, in which a multivariate function is applied many times with one
argument fixed, arises frequently. 
\crem{?? Many algorithm design techniques revolve around the idea of finding the
computations in $f$ that depend only on $x$, and
performing them only once.
\\
?? It's advantageous to find the computations in f that depend on x and perform
them only once.}
This is the intent of compiler optimization techniques like loop hoisting and
common subexpression elimination, which identify the computations common to all
invocations of $f$, and perform them once at the beginning of the computation.

Programs structured in this way are particularly important in real-time computer
graphics; in fact, modern graphics architectures \emph{require} that graphics
computations be organized into distinct {\em passes} which perform increasingly
fine-grained computations (for example, per object, per region of the screen,
and per pixel, where later computations use the results from the prior
computations).

While performant, this paradigm of programming with explicit collections of
passes results in complex code where invariants must hold across different
passes, and local changes for one pass may require changes to other passes to
``plumb'' inputs from all other frequencies\,\cite{Foley:2011}.
%
In other words, requiring manual separation of passes breaks key programming
abstractions such as composition and modularity.
%
Graphics researchers therefore have suggested using explicitly staged programs
\,\cite{Proudfoot:2001,Foley:2011,He:2014}, deferring to the compiler the
mechanical task of separating such programs into passes. However, prior work in
automatic pass separation has been limited to simple languages that do not
support recursion or first-class functions.

Programming languages researchers have studied similar problems. In the 1980s,
J{\o}rring and Scherlis identified {\em frequency reduction} and {\em
precomputation}~\cite{JS86-staging} as mechanisms for efficiently evaluating
staged computations. To perform frequency reduction, one identifies and hoists
computations that are performed multiple times, in order to compute them only
once. To perform precomputation, one identifies computations that can be
performed in advance, and does so---for example, at compile time if the relevant
inputs are statically known.

There has been much work on language-based techniques for performing
precomputation. Partial evaluation~\cite{futamura71,jones96} achieves
precomputation by specializing functions to specific arguments. Going back to
our example, if we specialize $f$ to a particular $v$, written $f_v$, such that
$f(v,y) = f_v(y)$, then we can specialize $F$ to $v$ as
\begin{lstlisting}
fun @$F_v(y_0, y_1, \ldots, y_{m-1})$@ = 
  @$f_v(y_0);  f_v(y_1); \ldots; f_v(y_{m-1})$@.
\end{lstlisting}
eliminating the need to recompute
\crem{WIP especially below here}
TODO.
%
A closely related technique, {\em metaprogramming}, allows writing programs that
generate specialized code. Metaprogramming enables fine-grained control over
specialization by requiring staging annotations that make explicit the stage of
each computation. (In partial evaluation, we typically assume that a
\emph{binding-time analysis} has automatically determined which parts of $f$ can
be specialized.)

While previous work shows that precomputation can be performed
essentially automatically (via partial evaluation or meta
programming), frequency reduction has been less explored.  In their
original paper, J{\o}rring and Scherlis proposed {\em pass separation}
as a manual method for {\em splitting} a program into multiple passes.
%
Applied to our example, pass separation would split the function $f$
into $f_1$ and $f_2$ such that $f(x,y) = f_2(f_1(x),y)$.  We can then
rewrite our example as a multi-pass program:
%
\begin{lstlisting}
fun @$F(x, y_0, y_1 \ldots y_{m-1})$@ = 
  let @$fx = f_1(x)$@
  in @$(f_2(fx, y_0);  f_2(fx, y_1)  \ldots f_2(fx, y_{m-1}))$@.
\end{lstlisting}
%
The key difference between partial evaluation (meta programming) and
pass separation is that the latter does not require the first
argument~$x$: it works with all arguments, whereas the partially
evaluated program is specialized for a particular value of $x$ ($x =
[1,2, \ldots, n]$ in the example). Specifically, if desired, partial
evaluation can be applied subsequently in our example, by partially
evaluating function~$f_2$ on $f_1(x)$.

However, unlike for precomputation, there is no available technique
for pass separation except for very simple languages
\cite{knoblock96,Proudfoot:2001,Foley:2011,He:2014}.  Notably omitted
features include recursion and first-class functions.  We are thus
interested in the problem of splitting automatically a functional
language.
%

As our starting point, we consider a modal, typed language \lang\ with explicit
staging in the style of Davies~\cite{davies96}, where the $\fut$
modality denotes computation in a later stage, and present a staged
operational semantics similar for \lang\ (\ref{sec:stagedsemantics}).
We then present a splitting algorithm that pass-separates a \lang\ program
into two monostaged programs, one for the first stage and for the
second stage (\ref{sec:splitting}).  Since each program is monostaged,
they are expressed in conventional functional languages.

The crux of the splitting algorithm is splitting recursive mixed stage
functions that involve both a first stage and second stage
computations.  When split, such a function turns into a first-pass
function that yields a {\em boundary value} that encodes the
``first-stage'' part of the computation.  If the function recurs on a
first-stage argument, then the boundary value is typically a recursive
data structure.  In other words, the splitting algorithm maps
computational recursion into a recursive data type. If the boundary
structure is generated as a result of a conditional, it is typically
encoded as a (recursive) sum type, such as a tree, where the kind of
the node indicates the recursion status of the first-stage evaluation.
The second-pass part of the computation, which corresponds to the
``second stage'' takes the boundary value as an argument as well as
the second-stage argument and uses them to complete the computation,
often by traversing the now precomputed boundary in light of the now
available second-stage argument.

Since it deals with unrestricted recursion and a rich language
consisting of first-class functions, recursion, and sum and recursive
types, the splitting algorithm is naturally complex. With some care,
the algorithm nevertheless can be specified reasonably succinctly. In
fact, we have implemented the algorithm (\ref{sec:implementation}) and
applied it to a number of staged programs (\ref{sec:examples}) ranging
from straight-line arithmetic operations to recursive and higher-order
functions. In the case of \texttt{quickselect}, which we discuss next, 
the split code is asymptotically faster over many $y_i$.

