\section{Introduction}

We often find ourselves in situations where the input to a computation arrives in two parts, at different points in time.  

The natural response is to try and do some part of the computation early, and the rest of the computation once the second input arrives.  

Since the computation runs in two parts, the total cost can be split as well, into $m+n$.

Additionally, we can run the second stage multiple times.  Cost is now $m+bn$.

Jorring et al. (\cite{jorring86}) identify three classes of staging techniques: meta-programming, partial evaluation, and stage-splitting.  The first two of these have received significantly more attention than the third.  Countless meta-programming systems exist (...twelve thousand citations...\cite{devito13}), and their background theory and type-systems are well understood (\cite{davies01}).  Partial evaluation, too, is well-understood.  Partial evaluation systems exist... . This paper explores both theory and applications for stage-splitting.  



\section{Stage-Splitting Definition and Comparison to Partial Evaluation}

First, we review the definition of partial evaluation.  Informally, a partial evaluator takes the code for a function $f$, as well as the first-stage input $x$ to that function, and produces the code for a version of that function {\em specialized} to the first input, often called $f_x$.  This $f_x$ function can then be evaluated with the second-stage input to produce the same final answer that $f$ would have.  The goal of the process is that $f_x$ should be cheaper to evaluate than $f$, although this can't be guaranteed for all inputs.  We now state this theorem more formally: a partial evaluator is some function $p$ such that,
\[
	\forall f,x. \exists f_x. [p(f,x) = f_x \text{ and } \forall y.\llbracket f \rrbracket(x,y)=\llbracket f_x \rrbracket(y)]
\]
where (borrowing notation from \cite{jones96}), $\llbracket f \rrbracket$ means the mathematical function corresponding to the code given by $f$.

Informally, we define stage-splitting to be the process of taking some function $f$ into two other functions, $f_1$ and $f_2$, where $f_1$ computes a partial result from the first-stage input, and $f_2$ uses that partial result and the second-stage input to compute a final result which is the same as if we had just run the original $f$ on both inputs.  Again, more formally, a stage-splitter is some $s$ such that,
\[
	\forall f. \exists f_1,f_2. [s(f) = (f_1,f_2) \text{ and } 
	\forall x,y.\llbracket f \rrbracket(x,y)=\llbracket f_2 \rrbracket(\llbracket f_1 \rrbracket(x),y)]
\]

We first discuss a few similarities between partial-evaluation and stage-splitting.  First off, both techniques have the same form of input, namely a bivariate function where the first input comes at stage one, and the second input comes at stage two.  

Again in both cases, the governing equations are too weak to fully determine the definitions of $p$ and $s$.  Indeed, both admit completely trivial definitions.  Consider the stage-splitter which always returns the identity for $f_1$ and $f$ for $f_2$, or analogously the partial evaluator which always returns an $f_x$ that just closes over the input $x$ and internally calls $f$ once $y$ is available. The ambiguity of these equations (modulo standard program equivalence of the outputs) can be resolved by adding annotations to $f$ to clearly specify the parts of the computation that are first stage and the parts that are second stage.  Later, we show that the same annotations suffice for both partial evaluation and stage-splitting.  

The differences between stage-splitting and partial evaluation are likewise evident from these governing equations.  For instance in partial evaluation, the existential $f_x$ depends on $x$, which means that the partial evaluator cannot be run until $x$ is known.  Moreover, if one wishes to specialize $f$ for multiple $x$'s, then the partial evaluator must be run several times.  Depending on the use case and cost of partial evaluation, this may be prohibitively expensive.  Alternatively, a stage-splitter need only be run once, and this can be done entirely before any $x$ is known.

\subsection{Partial Evaluator from Stage-Splitter}

We can recover a valid partial evaluator from a stage-splitter by stage-splitting the input function $f$ into $f_1$ and $f_2$, computing $\llbracket f_1 \rrbracket(x)$ to obtain $\bar x$, and then returning an $f_x$ such that $\llbracket f_x \rrbracket(y) = \llbracket f_2 \rrbracket(\bar x, y)$.  Note that this does not mean that stage-splitting is a strict generalization of partial evaluation.  In practice, partial evaluators easily perform optimizations (such as branch elimination, discussed later) which are beyond the scope of stage-splitting, and would require further technology than has been developed here.  It is best to think of stage-splitting as simply the first half of partial evaluation, where the back half is an optimizer. [Might be able to come up with a futamura projection-like statement here, which would be really really really cool.]

\subsection{Stage-Splitter from Partial Evaluator}

Likewise, we can easily recover a stage-splitter from a partial evaluator.  If $p$ is a valid partial evaluator, then we can define a stage splitter $s$ such that $s(f)=(f_1,f_2)$, where
\begin{align*}
\llbracket f_1 \rrbracket (x) &= p (f,x) \\
\llbracket f_2 \rrbracket (l,y) &= [l] (y)
\end{align*}
This implicitly requires that the languages in which $f_1$ and $f_2$ are expressed are strong enough to write a partial evaluator, but that is the case in this paper.  A stage-splitter defined this way leaves much to be desired.  Firstly, partial evaluation of $f$ may be too expensive for the context in which $f_1$ needs to run.  Additionally, the intermediate data structure created this way may be much larger than necessary, as it would contain all of the residual code.