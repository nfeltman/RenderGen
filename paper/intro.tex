%!TEX root = paper.tex

\section{Introduction}
\label{sec:intro}

\input{figures/quickselect}
Consider a function $F$ which takes an $x$ and $y_1,\dots,y_m$, and computes
$f(x,y_i)$ for each $y_i$:
\begin{lstlisting}
fun @$F(x, y_1, y_2 \ldots, y_m)$@ = 
  @$f(x, y_1);  f(x, y_2); \ldots; f(x, y_m)$@
\end{lstlisting}
%
In this scenario, a multivariate function is applied many times with
one argument fixed, so it is advantageous to find the computations in
$f$ that depend on $x$ and then stage program execution so they are
performed only once at the beginning of execution.

%Programs featuring this structure are particularly important in real-time
%computer graphics; in fact, modern graphics architectures \emph{require} that
%graphics computations be organized into distinct {\em passes} which perform
%increasingly fine-grained computations (for example, per object, per region of
%the screen, and per pixel, where many computations in later passes use the
%results from a single computation in an earlier pass)\,\cite{OpenGL4Spec}.
%While performant, this paradigm of programming with explicit passes results in
%complex code where invariants must hold across different passes, and local
%changes for one pass may require changes to other passes to ``plumb'' inputs
%from all other frequencies (\TODO explain what ``frequency'' means here)\,\cite{Foley:2011}.
%In other words, requiring manual separation of passes breaks key programming
%abstractions such as composition and modularity.
%Graphics researchers therefore have suggested using explicitly staged programs
%\,\cite{Proudfoot:2001,Foley:2011,He:2014}, deferring to the compiler the
%mechanical task of separating such programs into passes.

%Programming languages researchers have studied similar problems, with
%J{\o}rring and Scherlis identifying {\em frequency reduction} and {\em
% precomputation} as mechanisms for efficiently evaluating staged
%computations\,\cite{JS86-staging}.

J{\o}rring and Scherlis classify automatic program staging
transformations, such as the one described above, as forms of {\em
  frequency reduction} or {\em
  precomputation}\,\cite{JS86-staging}. To perform frequency
reduction, one identifies and hoists computations that are performed
multiple times, in order to compute them only once. To perform
precomputation, one identifies computations that can be performed in
advance and does so---for example, at compile time if the relevant
inputs are statically known.

One common precomputation technique is partial
evaluation\,\cite{futamura71,jones96}, which relies on dynamic
compilation to specialize functions to known argument values. Going
back to our example, if $f$ specializes to a particular $v$, written
$f_v$, such that $f(v,y) = f_v(y)$, then $F$ can be specialized to $v$
as
\begin{lstlisting}
fun @$F_v(y_1, y_2, \ldots, y_m)$@ = 
  @$f_v(y_1);  f_v(y_2); \ldots; f_v(y_m)$@.
\end{lstlisting}
eliminating the need to compute $m$ times those parts of $f(v,-)$
which do not depend on the second argument.

Closely related to partial evaluation is {\em metaprogramming}, where
known values represent program code to be executed in a later stage
\,\cite{davies96,Taha97,DP01-modal,NP05-nn}. Metaprogramming enables
fine-grained control over specialization by requiring explicit
\emph{staging annotations} that mark the stage of each expression.

%(In partial evaluation, we typically assume that a
%\emph{binding-time analysis} has automatically determined which parts
%of $f$ can be specialized.)

%This is the intent of compiler optimization techniques like loop
%hoisting and common subexpression elimination, which identify the
%computations common to all invocations of $f$, and perform them once
%at the beginning of the computation.

%While partial evaluation and metaprogramming have been studied
%extensively, frequency reduction techniques have not seen the same
%attention.

While simple forms of frequency reduction include standard compiler
optimizations such as loop hoisting and common subexpression
elimination, J{\o}rring and Scherlis proposed the more general
transformation of {\em splitting} a program into multiple subfunctions
(called pass separation in \,\cite{JS86-staging}). In our example,
splitting transforms the function $f$ into two others $f_1$ and $f_2$
such that $f(x,y_i) = f_2(f_1(x),y_i)$. Then we evaluate $F$ by
evaluating $f_1$ on $x$, and using the result $z$ to evaluate
the second function on each $y_i$.
%
\begin{lstlisting}
fun @$F_\textrm{multipass}(x, y_1, y_2, \ldots, y_m)$@ = 
  let @$z = f_1(x)$@
  in @$f_2(z, y_1);  f_2(z, y_2); \ldots; f_2(z, y_m)$@
\end{lstlisting}
%
The key difference between splitting and partial evaluation (or
metaprogramming) is that the former can be performed without access to
the first argument $x$; $F_\textrm{multipass}$ works for any $x$,
while $F_v$ is defined only for $x=v$.  Therefore, unlike partial
evaluation, splitting is a static program transformation
(``metastatic'' in partial evaluation terms) and does not require
dynamic code generation.

% Note that unlike precomputation, frequency reduction does not require
%knowledge of arguments; however, if a particular $v$ is known, we can
%use partial evaluation to specialize $F_\textrm{multipass}$ to that
%$v$.

Prior work on partial evaluation and metaprogramming has demonstrated
automatic application of these techniques on higher order functional
languages. In contrast, automatic splitting transformations have been
limited to simpler languages\,\cite{knoblock96,Proudfoot:2001,Foley:2011,He:2014}. 

In this paper, we present a splitting algorithm for \lang, 
a two-staged typed lambda calculus in the style of Davies\,\cite{davies96},
with support for recursion and first-class functions.
Like Davies, \lang\ uses a $\fut$ modality to denote computation in the second stage,
but to aid splitting we also add a $\curr$ modality to denote purely first-stage computations. 
The dynamic semantics (\ref{sec:semantics}) of \lang\ are that of Davies, 
modified to provide an eager behavior between stages, which we believe is more intuitive in the context of splitting.
We then prove the correctness of our splitting algorithm (\ref{sec:splitting}) for \lang\ with respect to the semantics.
Finally, we discuss our implementation of this splitting algorithm
(\ref{sec:implementation}) and demonstrate its power and behavior
for a number of staged programs ranging from straight-line arithmetic
operations to recursive and higher-order functions (\ref{sec:examples}).

We also demonstrate that splitting a recursive mixed-stage $f$
yields an $f_1$ which computes a recursive data structure
and an $f_2$ which traverses that structure
in light of information available at the second stage.
In the case of the quickselect algorithm, which we discuss next, 
the split code executes asymptotically faster than an unstaged evaluation of $f$.

