
\section{Umut's notes}

...To be added to the paper...
(TODO: make a pass to make consistent the terminology. it is a bit complex.)

Answer the following question for all of the related work:

What happens when we try to write quickselect as a meta program? 

\newcommand{\drun}[2]{\lVert{#2}\rVert_{#1}}

\subsection{Mixed Computation, Partial Evaluation, and Data Specialization.}

\paragraph{Defitions and relationships.}
The following definitions are adapted from the partial evaluation
book, the cited papers, and Malmkjaer thesis.  For simplicity and
uniformity, we assume that data and all programs are drawn from the
same set and that ill-behaved programs and non-terminating programs
all return $bottom$.

Definition[Residual].  

Let $p \in P$ be a program in a language $P$, which takes two inputs
$d_s \in P$ (static data) and $d_d \in P$ (dynamic data).  The program
$r \in P$ is a {\em residual for $p$ with respect to $d_s$} if for all
$d_d$, residual behaves that same as the $p$ for static data $d_s$,
i.e., 
\[
\drun{P}{p} (d_s, d_d) = \drun{P}{r} (d_d).
\]


Definition[Partial Evaluator]
A {\em partial evaluator} or a {\em program specializer} is a program
$e \in P$ (for ``evaluate'') such that for every $p \in P$, and every
$d_s, d_d \in P$, 
\[
\drun{P}{p} (d_s, d_d) = \drun{P}{\drun{P}{e}(p, d_s)} (d_d).
\]

Foundations of partial evaluation go back to Kleene's s-n-m theorem
from 1952, which constructively proved the existence of partial
evaluators by using Turing machines~\cite{Kleene52}.  Kleene did not
intend to improve efficiency, however. The first use of the term
``partial evaluation'' appears in Lombardi and Raphael (1964), in the
context of a paper on incremental camputation. Futamura (1971)
considered self-application of partial evaluation, also using it for
the purpose of compiler generation.  Ershov enriched Futamura's work
by considering double self-application, which led to compiler
generator generators, calling also Futamura's equations between
partially evaluated programs as ``Futamura projections.''  


\paragraph{Staging and staging transformations.}

To the best of our knowledge, Jorring and Sherlis are the
first to define precisely the notion of {\em staging} by recognizing
it to be a common optimization performed in software design, as the
programmer tries to stage computations based on frequency of a
computation and the availability of data~\cite{jorring86}.  They then
define, staging as the problem of taking a function of the form
$f(x,y)$ and deriving two functions $f_1$ and $f_2$ such that $f(x,y)
= f_2(f_1(x),y)$.  As the authors point out, the key difference
between staging and partial evaluation as that the latter performs no
a priori transformation; instead when $x$ becomes available it
specialized $f(x,\cdot)$ as much as possible.  Based on this notion of
staging, Jorring and Sherlis also define the {\em meta programming}
approach where $f(x, \cdot)$ is specialized by a meta program derived
from the definition of $f(x,y)$.

Having defined staging and meta programming problems, Jorring and
Scherlis move on to describe techniques for them.  They present the
technique called {\em pass separation} as a solution the problem of
splitting a function into two such that the stage-1 function produces
an intermediate data structure from the stage-1 data, which is then
used by the stage-2 function along with stage-2 data to produce the
final result.  Jorring and Scherlis then note describe how
pass-separation can be applied to an interpreter to obtain a compiler
from it by manually transforming the interpreter code into a compiler
by analysis. They also describe how meta programming can be used to
derive a specialized program not by creating a data structure as pass
separation does by generating a more optimized code that ``freezes''
certain actions to be performed at run time. 

Jorring and Scherlis highlight that their techniques are not meant to
be applied automatically and that they describe what can be called a
``vision.''  Indeed they describe their approach by way of examples as
transformations that can be applied by the programmer manually and
point out that they are powerful techniques that will require some time
and further research to automate.

Our work provides strong evidence that pass-separation transformations
can indeed be performed on explicitly staged programs automatically.

\ur{I didn't understand meta programming transformations that they do
  and the relationship to followup work.}



Data specialization (Barzdin and Bulyonkov 88) is a generalization of
partial evaluation.

Definition [Data Specialization].  A data specializer for a language
$P$ consist of two algorithms $ds_p, ds_d \in P$ for specializing the
data and the program such that for any program $p \in P$ and any
static and dynamic data $d_s, d_d \in P$, the code-specialized program
when run with static-data-specialized program behaves the same as the
original program run with the original data, i.e.,

\[
\drun{P}{p} (d_s, d_d) = \drun{P}{\drun{P}{ds_p} (p)} (\drun{P}{ds_d}
(p,d_s), d_d)).
\]

Seeing that partial evaluation is a special case of data
specialization is not trivial, as it requires using an interpreter for
the language $P$ (written in $P$).  Specifically, we can use a partial
evaluator in place of data specializer $ds_d$ and an  interpreter for $P$ in
place of the program specializer $ds_p$. 




THIS IS IMPORTANT. EXPAND ON THIS.  Knoblock and Ruf's data
specialization is similar but statically splits the code. This is
important because it is not feasible to perform splitting at run time
as can be possible according to the definition of data splitting. This
is similar to ours.  Their approach, however, seems to assume that the
specialized code is a single non-recursive procedure.


Both partial evaluation and data specialization are special cases of
mixed computation, which allows both the program and the data to be
specialized with respect to each other.  Ershov introduced {\em mixed
  computation}. 


Definition [Mixed computation]. A mixed-computation for a language $P$
consist of two algorithms $m_p, m_d \in P$ for specializing the data
and the program such that for any program $p \in P$ and any data $d
\in P$ the specialized program run with the specialized data behaves
the same as the original program run with the original data, i.e.,

\[
\drun{P}{p} (d) = \drun{P}{\drun{P}{m_p} (p,d)} (\drun{P}{m_d} (p,d)).
\]


Partial evaluation is trivially a special case of mixed computation,
because it allows a transformation of the program only on the static
data, and because it allows only a simple transformation on data (the
extraction of the dynamic data).  Similarly data specialization is a
special case of mixed computation because it allows the program to be
specialized in the absence of data and only the static portion of the
data to be specialized with respect to the program.


\paragraph{Techniques}

Polyvariant.

Call Unrolling.

Elimination of duplicate work.


