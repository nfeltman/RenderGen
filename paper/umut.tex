\section{Related Work}

As Jorring and Scherlis argue frequency reduction and precomputation
are common techniques for both designing algorithms and performing
compiler optimizations~\cite{JS86-staging}.  The idea behind
frequency reduction is to identify computations that are performed
multiple times and pull them ahead so that they can be performed once
and used as needed later.  The idea behind precomputation is to
identify computations that can be performed earlier, for example at
compile time if their inputs are available statically and perform them
at that earlier time. It is not difficult to see that many interesting
algorithm design techniques and compiler optimizations do exactly
frequency reduction and precomputation.  For example, dynamic
programming and loop hoisting are a frequency reduction techniques,
whereas dynamic data structures and incremental computation are
frequency reduction techniques.

Perhaps one of the most well studied example of precomputation is
partial evaluation.  Partial evaluation distinguishes between a static
(compile-time) and dynamic (run-time) stage. Given a program and the
values of its static inputs, which are the subset of the inputs that
become available at compilation time, partial evaluation generates a
specialized program by performing the computations that depend on the
statics inputs~\cite{jones96}.  The roots of partial evaluation go
back to Kleene, who in 1952 proved that a multivariate program
(function) $f(x_1, \ldots, x_{n+m})$ can be specialized for any given
$a_1 \ldots a_m$ to obtain another program $g(x_1, \ldots, x_n)$ such
that $f(a_1, \ldots, a_m, y_1, \ldots, y_n) = g(y_1, \ldots,
y_n)$~\cite{Kleene52}.  Kleene did not intend to improve efficiency,
however.  Lombardi appears to be the first person to use the term in
his work on incremental computation~\cine{Lombardi67}.  Since then,
there has a lot of work on partial evaluation, the only broad-strokes
of which we can mention in this here.  We refer the interested reader
to the excellent book by Jones, Gommard, and Sestoft for a
comprehesive discussion of work until early 90's~\cite{JGS93}.

At a high level, earlier approaches to partial evaluation can be
viewed as operating in two stages: binding time analysis and program
specialization.  For a multivariate program, e.g., $f(x_s,x_d)$, with
clearly marked static and dynamic arguments, binding-time analysis
identifies all the parts of the program that can be computed by the
knowledge of static arguments. Using this information and the static
arguments themselves, program specialization specializes the program
to a partially evaluated program that can be used with many different
dynamic arguments.  This approach has been applied to construct
partial evaluators for a number of languages such as 
Scheme~\cite{OB91-Similix,Consel88-Schism}, considering for the most
part first-order subsets.

Subsequence experience with binding time analysis based approach
showed that it can be difficult to control, leading to specialized
programs whose performance can be difficult to predict. This led to
investigations based on type systems for making explicit in the
program the stage of each
computation~\cite{GJ91-lambda,NN92-twolevel}.  Davies~\cite{Davies96}
presented a logical construction of binding-time type systems by
deriving a type system via the Curry-Howard isomorphism applied to
temporal logic.  In subsequence work, Davies and Pfenning proposed a
new type system for staged computation based on a particular fragment
of modal logic~\cite{DP01-modal}.  These later approaches can be
viewed more generally meta programming, where evaluating a program at
a certain stage with its inputs at that stage gives another program to
be executed at the next stage (e.g.,~\cite{JS86-staging}). The work on
MetaML scaled type-based ideas to a full-scale language by developing
statically typed programming language based on ML that enables the
programmer to express programs with multiple
stages~\cite{Taha97,taha-thesis-99}.  MetaML's type system is similar
to Davies~\cite{Davies96} but extends it in several important ways.
Nanevksi and Pfenning further extended the these techniques by
allowing free variable to occur within staged
computations~\cite{NP05-nn}.

The type-system component of our work is closely related to this later
line of work on metaprogramming and staged computation.  The specific
extension to the typed lambda calculus that we use here is based on
the circle-modality of Davies~\cite{DP01-modal}.  Our types differ in
small ways to account for the specific evaluation model that we
propose.  The key difference between our work and this prior work is
the staging or the splitting algorithm, which given a two-staged
program splits it into two stages that can be executed sequentially,
passing information from the earlier to the latter stage by
construction. 
%
\ur{Do prior work evaluate first-stage expressions inside second stage
expressions?}
%
Unlike in partial evaluation, our splitting algorithm requires no
knowledge of the arguments themselves. The only information that is
required is their staging. Using just the staging information, the
splitting algorithm can generate programs that can perform non-trivial
transformations on the code automatically.  As our examples illustrate
such staging transformations can improve the efficiency of the source
program, sometimes in asymptotically significant ways.  
We are not aware of prior work in the context of higher-order
functional languages that presents such a splitting algorithm.

Perhaps the closest work to ours is the work on ``data
specialization'' by Knoblock and Ruf~\cite{knoblock96}.  As our work,
Knoblock and Ruf are concerned about the problem of splitting a
program into two stages.  They only consider, however, a very simple
first-order language and straightline programs.


\begin{comment}


\section{Umut's notes}

...To be added to the paper...
(TODO: make a pass to make consistent the terminology. it is a bit complex.)

Answer the following question for all of the related work:

What happens when we try to write quickselect as a meta program? 

\newcommand{\drun}[2]{\lVert{#2}\rVert_{#1}}

\subsection{Mixed Computation, Partial Evaluation, and Data Specialization.}

\paragraph{Defitions and relationships.}
The following definitions are adapted from the partial evaluation
book, the cited papers, and Malmkjaer thesis.  For simplicity and
uniformity, we assume that data and all programs are drawn from the
same set and that ill-behaved programs and non-terminating programs
all return $bottom$.

Definition[Residual].  

Let $p \in P$ be a program in a language $P$, which takes two inputs
$d_s \in P$ (static data) and $d_d \in P$ (dynamic data).  The program
$r \in P$ is a {\em residual for $p$ with respect to $d_s$} if for all
$d_d$, residual behaves that same as the $p$ for static data $d_s$,
i.e., 
\[
\drun{P}{p} (d_s, d_d) = \drun{P}{r} (d_d).
\]


Definition[Partial Evaluator]
A {\em partial evaluator} or a {\em program specializer} is a program
$e \in P$ (for ``evaluate'') such that for every $p \in P$, and every
$d_s, d_d \in P$, 
\[
\drun{P}{p} (d_s, d_d) = \drun{P}{\drun{P}{e}(p, d_s)} (d_d).
\]

Foundations of partial evaluation go back to Kleene's s-n-m theorem
from 1952, which constructively proved the existence of partial
evaluators by using Turing machines~\cite{Kleene52}.  Kleene did not
intend to improve efficiency, however. The first use of the term
``partial evaluation'' appears in Lombardi and Raphael (1964), in the
context of a paper on incremental camputation. Futamura (1971)
considered self-application of partial evaluation, also using it for
the purpose of compiler generation.  Ershov enriched Futamura's work
by considering double self-application, which led to compiler
generator generators, calling also Futamura's equations between
partially evaluated programs as ``Futamura projections.''  


\paragraph{Staging and staging transformations.}

To the best of our knowledge, Jorring and Sherlis are the
first to define precisely the notion of {\em staging} by recognizing
it to be a common optimization performed in software design, as the
programmer tries to stage computations based on frequency of a
computation and the availability of data~\cite{jorring86}.  They then
define, staging as the problem of taking a function of the form
$f(x,y)$ and deriving two functions $f_1$ and $f_2$ such that $f(x,y)
= f_2(f_1(x),y)$.  As the authors point out, the key difference
between staging and partial evaluation as that the latter performs no
a priori transformation; instead when $x$ becomes available it
specialized $f(x,\cdot)$ as much as possible.  Based on this notion of
staging, Jorring and Sherlis also define the {\em meta programming}
approach where $f(x, \cdot)$ is specialized by a meta program derived
from the definition of $f(x,y)$.

Having defined staging and meta programming problems, Jorring and
Scherlis move on to describe techniques for them.  They present the
technique called {\em pass separation} as a solution the problem of
splitting a function into two such that the stage-1 function produces
an intermediate data structure from the stage-1 data, which is then
used by the stage-2 function along with stage-2 data to produce the
final result.  Jorring and Scherlis then note describe how
pass-separation can be applied to an interpreter to obtain a compiler
from it by manually transforming the interpreter code into a compiler
by analysis. They also describe how meta programming can be used to
derive a specialized program not by creating a data structure as pass
separation does by generating a more optimized code that ``freezes''
certain actions to be performed at run time. 

Jorring and Scherlis highlight that their techniques are not meant to
be applied automatically and that they describe what can be called a
``vision.''  Indeed they describe their approach by way of examples as
transformations that can be applied by the programmer manually and
point out that they are powerful techniques that will require some time
and further research to automate.

Our work provides strong evidence that pass-separation transformations
can indeed be performed on explicitly staged programs automatically.

\ur{I didn't understand meta programming transformations that they do
  and the relationship to followup work.}


Data specialization (Barzdin and Bulyonkov 88) is a generalization of
partial evaluation.

Definition [Data Specialization].  A data specializer for a language
$P$ consist of two algorithms $ds_p, ds_d \in P$ for specializing the
data and the program such that for any program $p \in P$ and any
static and dynamic data $d_s, d_d \in P$, the code-specialized program
when run with static-data-specialized program behaves the same as the
original program run with the original data, i.e.,

\[
\drun{P}{p} (d_s, d_d) = \drun{P}{\drun{P}{ds_p} (p)} (\drun{P}{ds_d}
(p,d_s), d_d)).
\]

Seeing that partial evaluation is a special case of data
specialization is not trivial, as it requires using an interpreter for
the language $P$ (written in $P$).  Specifically, we can use a partial
evaluator in place of data specializer $ds_d$ and an  interpreter for $P$ in
place of the program specializer $ds_p$. 




THIS IS IMPORTANT. EXPAND ON THIS.  Knoblock and Ruf's data
specialization is similar but statically splits the code. This is
important because it is not feasible to perform splitting at run time
as can be possible according to the definition of data splitting. This
is similar to ours.  Their approach, however, seems to assume that the
specialized code is a single non-recursive procedure.


Both partial evaluation and data specialization are special cases of
mixed computation, which allows both the program and the data to be
specialized with respect to each other.  Ershov introduced {\em mixed
  computation}. 


Definition [Mixed computation]. A mixed-computation for a language $P$
consist of two algorithms $m_p, m_d \in P$ for specializing the data
and the program such that for any program $p \in P$ and any data $d
\in P$ the specialized program run with the specialized data behaves
the same as the original program run with the original data, i.e.,

\[
\drun{P}{p} (d) = \drun{P}{\drun{P}{m_p} (p,d)} (\drun{P}{m_d} (p,d)).
\]


Partial evaluation is trivially a special case of mixed computation,
because it allows a transformation of the program only on the static
data, and because it allows only a simple transformation on data (the
extraction of the dynamic data).  Similarly data specialization is a
special case of mixed computation because it allows the program to be
specialized in the absence of data and only the static portion of the
data to be specialized with respect to the program.


\paragraph{Techniques}

Polyvariant.

Call Unrolling.

Elimination of duplicate work.
\end{comment}

