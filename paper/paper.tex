\documentclass{article}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{qtree}
\usepackage{amssymb,amsthm}
\usepackage{cite}
\usepackage[left=3cm,top=3cm,right=4cm,nohead,bottom=3cm]{geometry}


\title{\Large\textbf{General Stage Untangling with Applications to Algorithm Derivation}}
\author{Nicolas Feltman et al.}
\begin{document}
\maketitle
\section{Introduction}
Often when programming high-performance systems, a programmer will wish to specialize a bivariate function $f : \alpha \times \beta \to \gamma$ to one of its inputs, $x : \alpha$, while leaving the other, $y : \beta$, for later.  This need may arise because $x$ varies at a higher frequency than $y$, and we would wish to avoid repeated calculation. It may also be the case that $x$ is available at a time when computational resources are less expensive than when $y$ becomes available.  It's even possible that knowledge of $x$ allows further code optimizations that would have otherwise been unwise to speculate.  

Consider two examples: the discrete Fourier transform (DFT) and a raytracer.  In the DFT case we say that $x : \mathbb{N}$ is the dimension of the transformation, $y : \mathbb{R}^x $ is the data to be transformed, and $f$ simply computes the DFT.  Much of the structure of modern DFT algorithms depends on $x$ (for both mathematical decomposition and cache performance reasons) but not on $y$, so a great deal of work could be done to specialize the algorithm to a particular number of dimensions.  In the raytracer case, we say that $x : \mathtt{Geom}$ is a specification of the scene geometry (perhaps a list of triangles), $y : \mathtt{Ray}$ is a ray cast from some point in the scene, and $f$ computes the first intersection of the ray and the geometry.  In modern raytracers, we may cast millions of rays without varying the scene geometry, so it could be advantageous to precompute as much of the algorithm as possible based on only scene geometry.

We say that the problems above exhibit a {\em staged} structure, in that they can be broken down into parts that run at different points in time ({\em i.e.}~stages).  There are a variety of techniques to take advantage of this staged structure.  The general theme is to do some work with the immedaite input ($x$), producing an intermediate result, and then finish the rest of the computation once the delayed input ($y$) is available.  We consider three techniques: program generation, partial evaluation, and pass separation.  Each technique has broader applicability than the previous, but is less aggressive of an optimization.

\subsection {Program Generation} 
A common solution is to write code that writes code.  Essentially, second-stage computations are represented by second-stage code, and second-stage code is a value that can be passed around and manipulated by first-stage code.  To use the terminology from above: this first stage code is a function that takes in $x$ and outputs second stage function, which itself takes in $y$ and outputs the final answer. Note that this is essentially a manual process, in that the programmer must know the function $f$ which is being specialized.  Using the equational notation of \cite{jones96}, where ``$\llbracket h \rrbracket~z$'' means the ``the code $h$ interpreted as a function and applied to input $z$," we say that a programmer defines a program generator $g$ that satisfies 
\[
\llbracket \llbracket g \rrbracket~x\rrbracket~y =\llbracket f \rrbracket~(x,y)
\] 
for all $x$ and $y$. When well supported by the language (\cite{devito13}, [some metaml citation]) this technique is often called {\em metaprogramming}.  Of course, even without first class support, one can represent the second stage code using strings or other data types.  Defined so broadly, program generation includes all compilers, where $f$ is a hypothetical interpretter, $g$ is the actual compiler, $x$ is a program in the source language, and $y$ is the input to $x$.  Program generation has also been used to implement the DFT example in the form of FFTW \cite{FFTW05}.  

The benefit of the program generation technique is full control, even up to the ability to use domain-specific optimizations.  This comes at the cost of essentially being a manual operation.  

\subsection {Partial Evaluation} 
The idea of partial evaluation is to {\em automatically} specialize the code of $f$ to the supplied input, in a manner similar to standard evaluation.  In particular this means defining a program $pe$, called a partial evaluator, to which we can pass the implementation of a function $f$ along with $f$'s immediate input $x$, and get back a version of $f$ that is {\em specialized} to $x$.  This specialized version, also called a {\em residual}, is often denoted as $f_x$.  Equationally, we have
\begin{align*}
\llbracket pe\rrbracket~(f,x) &= f_x \\
\llbracket f_x\rrbracket~y &= \llbracket f\rrbracket~(x,y)
\end{align*}
 for all $f$, $x$, and $y$.  As seminally observed by \cite{futamura71}, partially evaluating an interpretter on a source program is equivalent to compiling that program to whatever language the interpretter was written in!  This has the benefit of being an automatic process, although the effective compiler is only as good as the partial evaluator.  Much of the subsequent work in partial evaluators has been with the purpose of chasing this goal.

Partial evaluation is a more general technique than program generation, in that the partial evaluator is defined once and works for all $f$s that need to be specialized.  Of course, the partial evaluator has no domain knowledge of the program it is splitting, so it cannot be nearly as aggressive as a program generator might be.  The two techniques are similar in that they both require the first input $x$ to do their specializing action.

\subsection {Pass Separation} 
The final technique, pass separation, is conceptually the simplest.  The idea is to define a program called $ps$, for {\em pass separator}. It works by cleaving the function $f:\alpha \times \beta \to \gamma$ into two functions $f_1 : \alpha \to \tau$ and $f_2 : \beta \times \tau \to \gamma$ for some type $\tau$, where $f_1$ builds a data structure (of type $\tau$) from $x$, and $f_2$ consumes that data structure as well as $y$ to produce the standard ouput.  In analogy to partial evaluation, we call $f_2$ the {\em residual}.  Equationally, this is
\begin{align*}
\llbracket pe\rrbracket~f &= (f_1,f_2) \\
\llbracket f_2\rrbracket~(\llbracket f_1\rrbracket~x,y) &= \llbracket f\rrbracket~(x,y)
\end{align*}
for all $f$, $x$, and $y$.  This technique, performed manually, is a common exercise for every programmer.  In can be considered the general form of common compiler optimizations such as loop hoisting, an example considered later.  Like partial evaluation, pass separation is an automatic operation, but unlike both partial evaluation and program generation, pass separation does not require the value of the first argument, $x$, to properly specialize $f$.  This makes pass separation the most widely applicable of the staging techniques, at the cost of being able to perform aggressive optimizations in the residual that might depend on the first argument. 

In this work we present a pass separation algorithm for a fragment of ML.  Specifically, our formulation can separate terms containing sums and recursion, which have not appeared previously in the stage seperation literature.  We anticipate that the algorithm will also be able to also separate this language extended with with first-class functions, although this is left as future work.  Additionally, our presentation is more explicitly type-motivated than those that have come before.

\section{Binding-Time Analysis and Stage Untangling}

The pass separation problem can naturally be decomposed into two subproblems:
\begin{itemize}
\item Decide which parts of the definition of $f$ belong to which stage and produce an annotated version of $f$.
\item Use the annotated version of $f$ to produce $f_1$ and $f_2$.  
\end{itemize}

An analogous form of this decomposition exists in the partial evaluation literature, wherein the first subproblem is known as {\em binding-time analysis}.  The second subproblem, in which the difference between pass separation and partial evaluation is more manifest, does not seem to yet have a name.  We call it {\em stage untangling}.  Unlike previous work in pass separation, we focuses entirely on the stage untangling problem by assuming that $f$ is given to us in an annotated form. 

Previous research has observed that the pass separation problem is inherently ambiguous in that the equations defined above do not fully specify the definition of $pe$.  That is, there can exist multiple ways to partition $f$ into $f_1$ and $f_2$ that satisfy $\llbracket f_2\rrbracket~(\llbracket f_1\rrbracket~x,y) = \llbracket f\rrbracket~(x,y)$.  For a trivial example, we can set $f_1$ to be the identity and $f_2$ to be $f$. Fortunately, this ambiguity is contained entirely within the binding-time analysis portion of pass separation; stage untangling is entirely determined.

\section{$\lambda^{12}$}

(Introduce the language, talk about its type system and maybe its evaluation.)

\section{Examples}

Some examples to get us comfortable with the language.

\subsection{loop hoisting}

\section{Stage Seperaion Algorithm}

Talk about the goal of the transformation.  Introduce the general form with open variables and a context, which is {\em not} the simplistic $f_1$, $f_2$ thing.  Stress the first image/second image/boundary type stuff.

\subsection {Type translation}

\subsection{Term translation}

State the main theorems we want to be true.  Refer to the 

\subsection{Functions}

Talk about why the current set up for functions alleviates the boundary-type guessing problem.

\subsection{Issue with sums}

Sums result in dependant types, which is cool.

\section {Implementation}

Not much to discuss.  It exsists.  Talk about how crufty the naive transformation is.

\section{Related Work in Stage Seperation}

Our starting language, $\lambda^{12}$, is essentially a two-stage version of $\lambda^\bigcirc$ from \cite{davies96}, but beefed up with products, sums, and fixed-points.  That work motivates $\lambda^\bigcirc$ as an image of linear temporal logic under the Curry-Howard correspondence, and it makes the argument that $\lambda^\bigcirc$ is a good model for binding time analysis.  Although $\lambda^\bigcirc$ was designed with partial evaluation in mind, it turns out to be a good model for pass separation as well.  In contrast, \cite{davies01} presents $\lambda^\Box$, which is based off of branching temporal logic.  As that work shows, $\lambda^\Box$ is a good system for program generation, although we find the closed-code requirement to be more restricting than we need.

Pass seperation was introduced in \cite{jorring}.  They focused on motivating pass separation as a technique for compiler generation, and hinted at wider applicability.  Their approach was not mechanized or automated in any sense, and their examples were separated by hand.  We believe that out work represents the fulfillment of their prediction that ``the [pass separation] approach will elude full automation for some time."

\cite{hannan94} also uses a pass separation technique for generating compiler/evaluator pairs from interpretters specified as term-rewrite systems.  This heavy restriction on the form of the input prevents generalization of their style of pass separation.  [I don't know enough about term-rewrite systems yet to be able to comment further.]  Their method, while amenable to mechanization, was not actually implemented.

\cite{knoblock} represents the first attempt at fully automatic pass separation for code approaching general-purpose.  In particular, they implemented pass separation for a C-like shading language including basic arithmetic and if statements.  The main goal of their work, like ours, was to minimize recomputation. That said, their starting language was sufficiently restricted for recursive boundary types to be inexpressible.  Given that memory is at a premium in shading languages, so much of that work was also focused on minimizing the memory-footprint of the boundary type.

\bibliography{paper}
\bibliographystyle{plain}

\end{document}



