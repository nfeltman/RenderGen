\documentclass{article}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{stmaryrd}
\usepackage{proof}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{cite}
\usepackage{mathpartir}
\usepackage[left=3cm,top=3cm,right=4cm,nohead,bottom=3cm]{geometry}

\input {macros}

\title{\Large\textbf{General Stage Untangling with Applications to Algorithm Derivation}}
\author{Nicolas Feltman et al.}
\begin{document}
\maketitle
\section{Introduction}

Many computational tasks are premised on the notion of multiple domains, places and times at which computation occurs.  Yet programming languages traditionally view their execution environments as monolithic, specifying the What and the How, but not the When nor the Where.   As always, where the language falls short, a programmer instead must bear the burden.  Inevitably, this comes at the cost of expressivity, safety, or both.  In this work, we present a language for specifying certain multi-domain computations, sacrificing neither the expresivity nor the safety that programmers have come to expect in single-domain languages.

Consider the programming of a pipeline system.  Generally in such a system, data comes in to one processor, is transformed to an intermediate result, and then fed to later processors, possibly with some buffering, duplication, and reording in between.  We can view an algorithm for such as system as one computation, split accross two spatial domains (processors).  For a concrete example, we draw inspiration from graphics systems.  Lets say that we want to map a function $f : \mathrm{PixelCoord} \to \mathrm{Color}$ over every pixel on a 1024x768 screen.   Due to geometric coherence inherent in the problem, it's often advantageous to compute some information at the level of coarse 32x32 tiles (e.g. a list of the scene primitives which interact with that part of the screen) whose results can be used to accelerate the final per-pixel calculation.  In adition to yielding algorithmic benefits, this decomposition strategy maps well to parallel hardware.

Often when programming high-performance systems, a programmer will wish to specialize a bivariate function $f : \alpha \times \beta \to \gamma$ to one of its inputs, $x : \alpha$, while leaving the other, $y : \beta$, for later.  This need may arise because $x$ varies at a higher frequency than $y$, and we would wish to avoid repeated calculation. It may also be the case that $x$ is available at a time when computational resources are less expensive than when $y$ becomes available.  Here we say that the calculation is split accross two temporal domains.  Pulling another example from graphics, we consider a raytracer wherein $x : \mathtt{Geom}$ is a specification of the scene geometry (perhaps a list of triangles), $y : \mathtt{Ray}$ is a ray cast from some point in the scene, and $f$ computes the first intersection of the ray and the geometry.  In modern raytracers, we may cast millions of rays without varying the scene geometry, so it could be advantageous to precompute as much of the algorithm as possible based on only scene geometry.

In addition to the partitioning of a calculation across multiple domains, the examples above exhibit one further similarity: communication between domains is one-directional.  In the spatial case, information always flows from one processor to the next, but never backwards.  In the temporal case too, information made available at the second time cannot be observed earlier (indeed this seems to be a defining feature of time, generally).  For this reason, we refer to the domains as stages.  This property, that information flow is acyclic, informs the rest of this paper.

[Talk about how languages without inherent support for domains must instead tackle the problem by writing various passes/shaders. This is bad because it's not compositional.  For instance, I cannot write a function that talks about multiple stages at once.]

[Because of the acyclic nature of the problem, we can take algorithms specified in our multi-domain language and tease them apart, essentially recovering the old way of doing things.  This is called stage untangling.  It gets us all the safety and expression without messing up the rest of the compiler.]

[Talk about standard partial evaluation and pass separation, then place our work in the second, modulo binding time analysis.]

%\subsection {Program Generation} 
%A common solution is to write code that writes code.  Essentially, second-stage computations are represented by second-stage code, and second-stage code is a value that can be passed around and manipulated by first-stage code.  To use the terminology from above: this first stage code is a function that takes in $x$ and outputs second stage function, which itself takes in $y$ and outputs the final answer. Note that this is essentially a manual process, in that the programmer must know the function $f$ which is being specialized.  Using the equational notation of \cite{jones96}, where ``$\llbracket h \rrbracket~z$'' means the ``the code $h$ interpreted as a function and applied to input $z$," we say that a programmer defines a program generator $g$ that satisfies 
%\[
%\llbracket \llbracket g \rrbracket~x\rrbracket~y =\llbracket f \rrbracket~(x,y)
%\] 
%for all $x$ and $y$. When well supported by the language (\cite{devito13}, [some metaml citation]) this technique is often called {\em metaprogramming}.  Of course, even without first class support, one can represent the second stage code using strings or other data types.  Defined so broadly, program generation includes all compilers, where $f$ is a hypothetical interpretter, $g$ is the actual compiler, $x$ is a program in the source language, and $y$ is the input to $x$.  Program generation has also been used to implement the DFT example in the form of FFTW \cite{FFTW05}.  
%
%The benefit of the program generation technique is full control, even up to the ability to use domain-specific optimizations.  This comes at the cost of essentially being a manual operation.  
%
%\subsection {Partial Evaluation} 
%The idea of partial evaluation is to {\em automatically} specialize the code of $f$ to the supplied input, in a manner similar to standard evaluation.  In particular this means defining a program $pe$, called a partial evaluator, to which we can pass the implementation of a function $f$ along with $f$'s immediate input $x$, and get back a version of $f$ that is {\em specialized} to $x$.  This specialized version, also called a {\em residual}, is often denoted as $f_x$.  Equationally, we have
%\begin{align*}
%\llbracket pe\rrbracket~(f,x) &= f_x \\
%\llbracket f_x\rrbracket~y &= \llbracket f\rrbracket~(x,y)
%\end{align*}
% for all $f$, $x$, and $y$.  As seminally observed by \cite{futamura71}, partially evaluating an interpretter on a source program is equivalent to compiling that program to whatever language the interpretter was written in!  This has the benefit of being an automatic process, although the effective compiler is only as good as the partial evaluator.  Much of the subsequent work in partial evaluators has been with the purpose of chasing this goal.
%
%Partial evaluation is a more general technique than program generation, in that the partial evaluator is defined once and works for all $f$s that need to be specialized.  Of course, the partial evaluator has no domain knowledge of the program it is splitting, so it cannot be nearly as aggressive as a program generator might be.  The two techniques are similar in that they both require the first input $x$ to do their specializing action.
%
%\subsection {Pass Separation} 
%The final technique, pass separation, is conceptually the simplest.  The idea is to define a program called $ps$, for {\em pass separator}. It works by cleaving the function $f:\alpha \times \beta \to \gamma$ into two functions $f_1 : \alpha \to \tau$ and $f_2 : \beta \times \tau \to \gamma$ for some type $\tau$, where $f_1$ builds a data structure (of type $\tau$) from $x$, and $f_2$ consumes that data structure as well as $y$ to produce the standard ouput.  In analogy to partial evaluation, we call $f_2$ the {\em residual}.  Equationally, this is
%\begin{align*}
%\llbracket pe\rrbracket~f &= (f_1,f_2) \\
%\llbracket f_2\rrbracket~(\llbracket f_1\rrbracket~x,y) &= \llbracket f\rrbracket~(x,y)
%\end{align*}
%for all $f$, $x$, and $y$.  This technique, performed manually, is a common exercise for every programmer.  In can be considered the general form of common compiler optimizations such as loop hoisting, an example considered later.  Like partial evaluation, pass separation is an automatic operation, but unlike both partial evaluation and program generation, pass separation does not require the value of the first argument, $x$, to properly specialize $f$.  This makes pass separation the most widely applicable of the staging techniques, at the cost of being able to perform aggressive optimizations in the residual that might depend on the first argument. 
%
%In this work we present a pass separation algorithm for a fragment of ML.  Specifically, our formulation can separate terms containing sums and recursion, which have not appeared previously in the stage seperation literature.  We anticipate that the algorithm will also be able to also separate this language extended with with first-class functions, although this is left as future work.  Additionally, our presentation is more explicitly type-motivated than those that have come before.
%
%\section{Binding-Time Analysis and Stage Untangling}
%
%The pass separation problem can naturally be decomposed into two subproblems:
%\begin{itemize}
%\item Decide which parts of the definition of $f$ belong to which stage and produce an annotated version of $f$.
%\item Use the annotated version of $f$ to produce $f_1$ and $f_2$.  
%\end{itemize}
%
%An analogous form of this decomposition exists in the partial evaluation literature, wherein the first subproblem is known as {\em binding-time analysis}.  The second subproblem, in which the difference between pass separation and partial evaluation is more manifest, does not seem to yet have a name.  We call it {\em stage untangling}.  Unlike previous work in pass separation, we focuses entirely on the stage untangling problem by assuming that $f$ is given to us in an annotated form. 
%
%Previous research has observed that the pass separation problem is inherently ambiguous in that the equations defined above do not fully specify the definition of $pe$.  That is, there can exist multiple ways to partition $f$ into $f_1$ and $f_2$ that satisfy $\llbracket f_2\rrbracket~(\llbracket f_1\rrbracket~x,y) = \llbracket f\rrbracket~(x,y)$.  For a trivial example, we can set $f_1$ to be the identity and $f_2$ to be $f$. Fortunately, this ambiguity is contained entirely within the binding-time analysis portion of pass separation; stage untangling is entirely determined.

\section{A Simple Two-Stage Language}

We'll start by analyzing a simple two-stage language called \corelang.  We say it is simple in that it supports boolean, integer, and product types, as well as let bindings, if expressions, and various primitive operations.  Notably, we leave out functions, fixed point operators, and full sum types, which will be added later.

The grammar for types, terms, values, and contexts in \corelang are given in Figure \ref{fig:coreSyntax}.  The first thing to notice is that stage $\bbone$ and stage $\bbtwo$ code are syntactically seperated at the type, term, and value levels, despite heavy overlap.  It would be tempting to merge the two stages into one syntactic class and maintain the stage invariants with the typing judgements.  Such a trick would reduce the total number of rules presented later, but at the cost of making those rules type derivation-directed, rather than syntax directed.  Alternatively, we chose to have one context merged accross both stages, rather than one context for each stage.  The intended effect of this decision is for bindings in one stage to shadow binding to the same variables in the other stage.  The reasons for this will become clear later.

At the type and value levels, stage $\bbone$ can refer to stage $\bbtwo$, but stage $\bbtwo$ cannot refer back to stage $\bbone$.  Alternatively, the two stages are mutually recursive at the term level.  These two patterns together fulfill the promise of {\em mixed code, staged evalutation}.

In the rest of this document, I will tend to use $\{A,B,C\}$ for types, $\{v,u,w,t\}$ for values, $\{x,y\}$ for variables, $e$ for terms, $i$ for integers, and $b$ for booleans.

\begin{figure}
\caption{\corelang~Syntax}
\label{fig:coreSyntax}
\centering
\begin{tabular}{ll} 
$\begin{aligned}
\typeo &::= \text{unit}~|~\text{int}~|~\text{bool} \\
&\gbar \typeo \times \typeo \\
&\gbar \fut \typet 
\end{aligned} $  
& 
$\begin{aligned}
\typet &::=  \text{unit}~|~\text{int}~|~\text{bool} \\
&\gbar \typet \times \typet \\
&
\end{aligned} $  
\\ 
$\begin{aligned}
\expro &::= ()~|~\inte~|~\bool  \\
&\gbar \letin{\var}{\expro}{\expro} \\
&\gbar \var \\
&\gbar (\expro, \expro) \\
&\gbar \pi_1~\expro \gbar \pi_2~\expro \\
&\gbar \ifthen {\expro}{\expro}{\expro} \\
&\gbar \next~\exprt 
\end{aligned} $ 
& 
$\begin{aligned}
\exprt &::= ()~|~\inte~|~\bool \\
&\gbar \letin{\var}{\exprt}{\exprt} \\
&\gbar \var \\
&\gbar (\exprt, \exprt) \\
&\gbar \pi_1~\exprt \gbar \pi_2~\exprt \\
&\gbar \ifthen {\exprt}{\exprt}{\exprt} \\
&\gbar \prev~\expro
\end{aligned} $
\\ 
$\begin{aligned}
\valo &::= ()~|~\inte~|~\bool \\
&\gbar \valprod {\valo} {\valo} \\
&\gbar \valnext~\valt
\end{aligned} $
& 
$\begin{aligned}
\valt &::= ()~|~\inte~|~\bool \\
&\gbar \valprod {\valt} {\valt} \\
& 
\end{aligned} $
\\
$\begin{aligned}
\contextot &::=\emptyC \\
&\gbar \contextot, \var : \typeo ^\bbone \\
&\gbar \contextot, \var : \typet ^\bbtwo
\end{aligned} $
\end{tabular}
\end{figure}

\subsection{Static Semantics}

The static semantics for \corelang~is given in Figure \ref{fig:coreStatics}.  Modulo the two-stage restriction and current absence of functions, it's based off of the system in \cite{davies96}, which was inspired by temporal logic (the source of our $\fut$ modality).  The static semantics comprises two judgements.  Those are:

\begin{center}
\begin{tabular}{|l|l|} \hline
Name & Pattern \\ \hline
Typing at $\bbone$ & $\typesone[\contextot]{\expro}{\typeo}$ \\  \hline
Typing at $\bbtwo$ & $\typestwo[\contextot]{\exprt}{\typet}$ \\ \hline
\end{tabular}
\end{center}

We see that the typing judgements for both stages are each what one would expect for an unstaged language.  That is, there are introduction and elimination forms for base types and products at both stage $\bbone$ and stage $\bbtwo$.  Furthermore, these rules preserve stage.  For these reasons, we can think of the base type and product features as essentially being orthogonal to the staging features.  The terms $\next$ and $\prev$, which are the only terms that allow us to change stage, are respectively the introduction and elimination forms for $\fut$.

\begin{figure}
\caption{\corelang~Static Semantics}
\label{fig:coreStatics}
\begin{mathpar}
\infertypesone [\rmunit] 			{\typesone {()}{\rm unit}}{\cdot} \and
\infertypesone [\mathrm{int}]		{\typesone {i}{\rm int}}{\cdot} \and
\infertypesone [\mathrm{bool}]	{\typesone {b} {\rm bool}}{\cdot} \and
\infertypesone [\mathrm{hyp}]		{\typesone x A}{x : A^\bbone \in \Gamma} \and
\infertypestwo [\rmunit]			{\typestwo {()}{\rm unit}}{\cdot} \and
\infertypestwo [\mathrm{int}]		{\typestwo {i}{\rm int}}{\cdot} \and
\infertypestwo [\mathrm{bool}]	{\typestwo {b} {\rm bool}}{\cdot} \and
\infertypestwo [\mathrm{hyp}]		{\typestwo x A}{x : A^\bbtwo \in \Gamma} \and
\infertypesone [\mathrm{let}]		{\typesone{\letin{x}{e_1}{e_2}}{B}}{\typesone{e_1} A & \typesone [\Gamma,x:A^\bbone] {e_2} B} \and
\infertypestwo [\mathrm{let}]		{\typestwo{\letin{x}{e_1}{e_2}}{B}}{\typestwo{e_1} A & \typestwo [\Gamma,x:A^\bbtwo] {e_2} B} \and
\infertypesone [\times\mathrm{I}]		{\typesone {(e_1,e_2)} {A\times B}}{\typesone {e_1} A & \typesone {e_2} B} \and
\infertypesone [\times\mathrm{E_1}]	{\typesone{\pi_1~e} A}{\typesone e {A\times B}} \and
\infertypesone [\times\mathrm{E_2}]	{\typesone{\pi_2~e} B}{\typesone e {A\times B}} \and
\infertypestwo [\times\mathrm{I}]		{\typestwo {(e_1,e_2)} {A\times B}}{\typestwo {e_1} A & \typestwo {e_2} B} \and
\infertypestwo [\times\mathrm{E_1}]	{\typestwo{\pi_1~e} A}{\typestwo e {A\times B}} \and
\infertypestwo [\times\mathrm{E_2}]	{\typestwo{\pi_2~e} B}{\typestwo e {A\times B}} \and
\infertypesone [\mathrm{if}] {\typesone{\ifthen{e_1}{e_2}{e_3}} A}{\typesone{e_1} {\rm bool} & \typesone {e_2} A & \typesone {e_3} A} \and
\infertypestwo [\mathrm{if}] {\typestwo{\ifthen{e_1}{e_2}{e_3}} A}{\typestwo{e_1} {\rm bool} & \typestwo {e_2} A & \typestwo {e_3} A} \and
\infertypesone [\fut\mathrm{I}]	{\typesone{\next~e}{\fut A}}{\typestwo e A} \and
\infertypestwo [\fut\mathrm{E}]	{\typestwo {\prev~e} A}{\typesone e {\fut A}} \and
\end{mathpar}
\end{figure}

\subsection{Erasure Dynamic Semantics}

The dynamics semantics for \corelang~is given in Figure \ref{fig:erasureSemantics} in big-step style.  Predictably, it comprises two judgements:

\begin{center}
\begin{tabular}{|l|l|} \hline
Name & Pattern \\ \hline
Reduction at $\bbone$ & $\reduceone{\expro}{\valo}$ \\  \hline
Reduction at $\bbtwo$ & $\reducetwo{\exprt}{\valt}$ \\ \hline
\end{tabular}
\end{center}

The primary goal for this semantics was simplicity.  This was achieved in that, like the static semantics, the judgements for each level are extremely similar to those for an unstaged language.  This simplicity came at the cost of some descriptiveness, however.  The inadequacy of this semantics will become apparent shortly.

First, we discuss a few desirable theorems.  Chiefly among them is type preservation (I haven't defined the judgements for the types of values, but it should be rather clear).  As will be the pattern for many theorems to come, our inductive hypothesis comprises two mutually dependent lemmas, one for stage $\bbone$, and the other for stage $\bbtwo$:
\begin{center}
\begin{tabular}{l}
If $\typesone [\emptyC]{e}{A}$ \\
and $\reduceone{e}{v}$ \\
then $v : A$
\end{tabular}
~~~
\begin{tabular}{l}
If $\typestwo [\emptyC]{e}{A}$ \\
and $\reducetwo{e}{v}$ \\
then $v : A$
\end{tabular}
\end{center}
[Proof not done, but it looks trivial in every case. [Wait no, substitutions kill me again.]]

We also want some formalization of the idea that stage two computation cannot affect stage one computation.  At a minimum, the following lemma should be true [this needs lots of work],

\begin{center}
\begin{tabular}{l}
If $\exists v\in \valt$ s.t. $\reduceone {[v/x]e}{u}$ \\
then $\forall v\in \valt$ s.t. $\reduceone {[v/x]e}{u'}$, $u \simeq u'$
\end{tabular}
\end{center}

As it turns out, this is not true! [Example forthcoming.]

\begin{figure}
\caption{\corelang~Erasure Dynamic Semantics}
\label{fig:erasureSemantics}
\begin{mathpar}
\inferreduceone [\rmunit]				{\reduceone {()}{()}}{\cdot} \and
\inferreduceone [\mathrm{int}]		{\reduceone{i}{i}}{\cdot} \and
\inferreduceone [\mathrm{bool}]		{\reduceone{b}{b}}{\cdot} \and
\inferreduceone [\mathrm{let}]			{\reduceone{\letin{x}{e_1}{e_2}}{v_2}}{\reduceonesub [1] & \reduceone{[v_1/x]e_2}{v_2}} \and
\inferreducetwo [\rmunit]				{\reducetwo {()}{()}}{\cdot} \and
\inferreducetwo [\mathrm{int}]		{\reducetwo{i}{i}}{\cdot} \and
\inferreducetwo [\mathrm{bool}]		{\reducetwo{b}{b}}{\cdot} \and
\inferreducetwo [\mathrm{let}]			{\reducetwo{\letin{x}{e_1}{e_2}}{v_2}}{\reducetwosub [1] & \reducetwo{[v_1/x]e_2}{v_2}} \and
\inferreduceone [\times\mathrm{I}]	{\reduceone{(e_1,e_2)}{\valprod{v_1}{v_2}}}{\reduceonesub [1] & \reduceonesub [2]} \and
\inferreduceone [\times\mathrm{E_1}]	{\reduceone{\pi_1~e}{v_1}}{\reduceone{e}{\valprod{v_1}{v_2}}} \and
\inferreduceone [\times\mathrm{E_2}]	{\reduceone{\pi_2~e}{v_2}}{\reduceone{e}{\valprod{v_1}{v_2}} } \and
\inferreducetwo [\times\mathrm{I}]	{\reducetwo{(e_1,e_2)}{\valprod{v_1}{v_2}}}{\reduceonesub [1] & \reduceonesub [2]} \and
\inferreducetwo [\times\mathrm{E_1}]	{\reducetwo{\pi_1~e}{v_1}}{\reduceone{e}{\valprod{v_1}{v_2}}} \and
\inferreducetwo [\times\mathrm{E_2}]	{\reducetwo{\pi_2~e}{v_2}}{\reduceone{e}{\valprod{v_1}{v_2}}} \and
\inferreduceone [\mathrm{if_1}] 		{\reduceone{\ifthen{e_1}{e_2}{e_3}}{v}}{\reduceone{e_1}{\bf true} & \reduceone{e_2}{v}} \and
\inferreduceone [\mathrm{if_2}] 		{\reduceone{\ifthen{e_1}{e_2}{e_3}}{v}}{\reduceone{e_1}{\bf false} & \reduceone{e_3}{v}} \\
\inferreducetwo [\mathrm{if_1}] 		{\reducetwo{\ifthen{e_1}{e_2}{e_3}}{v}}{\reducetwo{e_1}{\bf true} & \reducetwo{e_2}{v}} \and
\inferreducetwo [\mathrm{if_2}] 		{\reducetwo{\ifthen{e_1}{e_2}{e_3}}{v}}{\reducetwo{e_1}{\bf false} & \reducetwo{e_3}{v}} \\
\inferreduceone [\fut\mathrm{I}]		{\reduceone{\next~e}{\valnext~v}}{\reducetwosub} \and
\inferreducetwo [\fut\mathrm{E}]		{\reducetwo{\prev~e} v}{\reduceone e {\valnext~v}} 
\end{mathpar}
\end{figure}

%\subsection{Better Dynamic Semantics}
%
%Introduce the dynamics semantics.  Figure \ref{fig:coreDynamics}.  It's small step.  We figured this out on our own, but it really should exist in the partial evaluation literature somewhere.
%
%\newcommand {\stepo} {\hookrightarrow_\bbone} 
%\newcommand {\stept} {\hookrightarrow_\bbtwo} 
%\newcommand {\pco} {~\mathrm{pc}\bbone} 
%\newcommand {\fco} {~\mathrm{fc}\bbone} 
%\newcommand {\pct} {~\mathrm{pc}\bbtwo} 
%\newcommand {\fct} {~\mathrm{fc}\bbtwo} 
%
%\begin{figure}
%\caption{\corelang~Partially Computed Values (pc$\bbone$ and pc$\bbtwo$)}
%\label{fig:coreDynamics}
%\begin{mathpar}
%\infer {() \pco}{\cdot} \and
%\infer {i \pco}{\cdot} \and
%\infer {b \pco}{\cdot} \and
%\infer {(v_1, v_2) \pco}{v_1 \pco & v_2 \pco} \and
%\infer {\next~e \pco}{e \pct} \\
%\infer {() \pct}{\cdot} \and
%\infer {i \pct}{\cdot} \and
%\infer {b \pct}{\cdot} \and
%\infer {(v_1, v_2) \pct}{v_1 \pct & v_2 \pct} \and
%\infer {\pi_i e \pct}{e \pct} \and
%\infer {x \pct}{\cdot} \and
%\infer {\letin{x}{e_1}{e_2} \pct}{e_1 \pct & e_2 \pct} \and
%\infer {\ifthen{e_1}{e_2}{e_3} \pct}{e_1 \pct & e_2 \pct & e_3 \pct} \and
%\infer {\prev~e \pct}{e \pco} 
%\end{mathpar}
%\end{figure}
%
%\begin{figure}
%\caption{\corelang~Fully Computed Values (fc$\bbone$ and fc$\bbtwo$)}
%\label{fig:coreDynamics}
%\begin{mathpar}
%\infer {() \fco}{\cdot} \and
%\infer {i \fco}{\cdot} \and
%\infer {b \fco}{\cdot} \and
%\infer {(v_1, v_2) \fco}{v_1 \fco & v_2 \fco} \and
%\infer {\next~e \fco}{e \fct} \\
%\infer {() \fct}{\cdot} \and
%\infer {i \fct}{\cdot} \and
%\infer {b \fct}{\cdot} \and
%\infer {(v_1, v_2) \fct}{v_1 \fct & v_2 \fct} 
%\end{mathpar}
%\end{figure}
%
%\begin{figure}
%\caption{\corelang~Transitions}
%\label{fig:coreDynamics}
%\begin{mathpar}
%\infer {\ifthen{e_1}{e_2}{e_3} \stepo \ifthen{e_1'}{e_2}{e_3}}{e_1 \stepo e_1'}  \\
%\infer {\ifthen{\tt true}{e_2}{e_3} \stepo e_2}{\cdot} \and 
%\infer {\ifthen{\tt false}{e_2}{e_3} \stepo e_3}{\cdot}  \\
%\infer {\ifthen{e_1}{e_2}{e_3} \stept \ifthen{e_1'}{e_2}{e_3}}{e_1 \stept e_1' & e_2 \pct & e_3 \pct}  \\
%\infer {\ifthen{\tt true}{e_2}{e_3} \stept e_2}{e_2 \pct & e_3 \pct} \and 
%\infer {\ifthen{\tt false}{e_2}{e_3} \stept e_3}{e_2 \pct & e_3 \pct} \\
%\infer {\letin{x}{e_1}{e_2} \stepo \letin{x}{e_1'}{e_2}}{e_1 \stepo e_1'} \and
%\infer {\letin{x}{e_1}{e_2} \stepo [e_1/x]e_2}{e_1 \fco} \and
%\infer {\letin{x}{e_1}{e_2} \stept \letin{x}{e_1'}{e_2}}{e_1 \stept e_1'} \and
%\infer {\letin{x}{e_1}{e_2} \stept [e_1/x]e_2}{e_1 \fct} \and
%\infer {x \downtwo x}{\cdot} \and
%\infer {\gds (e_1,e_2) : A\times B}{\gds e_1 : A & \gds e_2 : B} \and
%\infer {\ifthen{e_1}{e_2}{e_3} \downone [v,r]}{e_1 \downone {\bf true} & e_2 \downone [v,r]} \and
%\infer {\ifthen{e_1}{e_2}{e_3} \downone [v,r]}{e_1 \downone {\bf false} & e_3 \downone [v,r]} \and
%\infer {\ifthen{e_1}{e_2}{e_3} \downtwo [v, \ifthen{r_1}{r_2}{r_3}]}{e_1 \downtwo [\valdum,r_1] & e_2 \downtwo [v_2,r_2] & e_3 \downtwo [v_3,r_3]} \and
%\infer {\next~e \downone [\valdum,r]}{e \downtwo r} \and
%\infer {\prev~e \downtwo r}{e \downone [\valdum, r]} \and
%\end{mathpar}
%\end{figure}

%\subsection{Good Theorems About the Language Alone}
%
%\begin{enumerate}
%\item Type Safety.
%\item Unidirectional Flow. [Essentially, you cannot write a program that goes back in time.]
%\item Erasure property.  [If you ignore the nexts and prevs, the result is weakly equivalent.] [This was proven for a denotational semantics in some Jones paper that I need to find...]
%\end{enumerate}

\subsection{Splitting}

We now cover splitting, the process by which mixed code is teased apart into two individual chunks of code, one for each stage.  To begin, we first introduce the target lagnuage (for both stage), \coremono.   The grammar for \coremono~is given in figure \ref{fig:baseSyntax}.  We see that \coremono~is (as one would hope) a traditional single-stage language, with all the same basetypes as \corelang, in addition to products, if expressions, and full sums (injections and case expressions).  The presence of this last feature is a bit curious, since \corelang~did not support full sums.  The necessity for sums should become apparent in time.  The static and dynamic semantics of \coremono~are not given, but should be obvious.  They are given by the judgements:

\begin{center}
\begin{tabular}{|l|l|} \hline
Name & Pattern \\ \hline
Typing & $\types{\expr}{\val}$ \\  \hline
Reduction & $\reduce{\expr}{\val}$ \\ \hline
\end{tabular}
\end{center}

The splitting judgements come in seven parts, two each for types, terms, and values, and one for contexts.  In tabular form, they are:
\begin{center}
\begin{tabular}{|l|l|} \hline
Name & Pattern \\ \hline
Value Splitting at $\bbone$& $\valo \vsplito [\val,\val]$ \\  \hline
Value Splitting at $\bbtwo$& $\valt \vsplits \val$ \\  \hline
Type Splitting at $\bbone$& $\typeo \tsplito [\type,\type]$ \\  \hline
Type Splitting at $\bbtwo$& $\typet \tsplits \type$ \\  \hline
Context Splitting & $\contextot \csplit [\context,\context]$ \\  \hline
Term Splitting at $\bbone$& $\splitone{\expro}{\typeo}{\expr,\var.\expr}$ \\  \hline
Term Splitting at $\bbtwo$& $\splittwo{\exprt}{\typet}{\expr,\var.\expr}$ \\  \hline
\end{tabular}
\end{center}

[...more...]

\begin{figure}
\caption{\coremono~Syntax}
\label{fig:baseSyntax}
\centering
\begin{tabular}{ll} 
$\begin{aligned}
\expr &::= ()~|~\inte~|~\bool \\
&\gbar \letin{\var}{\expr}{\expr} \\
&\gbar \var \\
&\gbar (\expr, \expr) \\
&\gbar \pi_1~\expr \gbar \pi_2~\expr \\
&\gbar \inl~\expr \gbar \inr~\expr \\
&\gbar \ifthen {\expr}{\expr}{\expr}  \\
&\gbar \caseof {\expr}{x_1.\expr}{x_2.\expr} 
\end{aligned} $
& 
$\begin{aligned}
\type &::=  \rmunit~|~\text{int}~|~\text{bool} \\
&\gbar \type \times \type  \\
&\gbar \type + \type 
\\
\val &::= ()~|~\inte~|~\bool \\
&\gbar \valprod {\val} {\val}  \\
&\gbar \valinl~\expr \gbar \valinr~\expr
\\
\context &::= \emptyC \\
&\gbar \context, \var : \type
\end{aligned} $
\end{tabular}
\end{figure}


\begin{figure*}
\caption{Type Splitting}
\label{fig:typeSplit}
\begin{mathpar}
\infer [\mathrm{unit}\tsplito]	{\rmunit \tsplito [\rmunit,\rmunit]}{\cdot} \and
\infer [\mathrm{int}\tsplito]	{{\rm int} \tsplito [{\rm int}, \rmunit]}{\cdot} \and
\infer [\mathrm{bool}\tsplito]	{{\rm bool} \tsplito [{\rm bool}, \rmunit]}{\cdot} \and
\infer [\times\tsplito]	
	{A\times B \tsplito [A_1 \times B_1, A_2 \times B_2]}
	{A \tsplito [A_1, A_2] & B \tsplito [B_1,B_2]} \and
\infer [\fut\tsplito]
	{\fut~A \tsplito [{\rm unit}, A']}
	{A \tsplits A'} \and
\end{mathpar}
\end{figure*}

\begin{figure*}
\caption{Value Splitting}
\label{fig:valSplit}
\begin{mathpar}
\infer [\rmunit\vsplito]		{() \vsplito [(), ()]}{\cdot} \and
\infer [\mathrm{int}\vsplito]	{i \vsplito [i,()]}{\cdot} \and
\infer [\mathrm{bool}\vsplito]	{b \vsplito [b,()]}{\cdot} \and
\infer [\times\vsplito]	
	{\valprod{v_1}{v_2} \vsplito [\valprod{u_1}{u_2}, \valprod{w_1}{w_2}]}
	{v_1 \vsplito [u_1,w_1] & v_2 \vsplito [u_2,w_2]} \and
\infer [\fut\vsplito]
	{\valnext~v \vsplito [(),v']}
	{v \vsplits v'} 
\end{mathpar}
\end{figure*}

\begin{figure*}
\caption{Context Splitting}
\label{fig:contSplit}
\begin{mathpar}
\infer {\emptyC \csplit [\emptyC~;~\emptyC]}{\cdot} \and
\infer {\Gamma, v : A^\bbone \csplit [\Gamma_1,v:A_1; \Gamma_2,v:A_2]}{\Gamma \csplit [\Gamma_1; \Gamma_2] & A \tsplito [A_1, A_2]} \and
\infer {\Gamma, v : A^\bbtwo \csplit [\Gamma_1; \Gamma_2,v:A']}{\Gamma \csplit [\Gamma_1; \Gamma_2] & A \tsplits A'} 
\end{mathpar}
\end{figure*}

\begin{figure*}
\caption{Term Splitting}
\label{fig:termSplit}
\begin{mathpar}
\infersplitone [\rmunit] 
	{\splitone{()}{\rmunit}{((),()),\_.()}} {\cdot} \and
\infersplittwo [\rmunit]
	{\splittwo{()}{\rmunit}{(),\_.()}} {\cdot} \and
\infersplitone [{\rm int}] 
	{\splitone{i}{\rm int}{(i,()),\_.()}} {\cdot} \and
\infersplittwo [{\rm int}]
	{\splittwo{i}{\rm int}{(),\_.i}} {\cdot} \and
\infersplitone [\times\mathrm{I}] 
	{\splitone {(e_1,e_2)}{A\times B}
		{ \begin{array}{l}
		\left(\letin{y_1}{c_1}{\letin{y_2}{c_2}{((\pi_1 y_1, \pi_1 y_2),(\pi_2 y_1, \pi_2 y_2))}}\right), \\
		l.(\letin{l_1}{\pi_1 l}{r_1},\letin{l_2}{\pi_2 l}{r_2})
		\end{array}}}
	{ \splitonesub [1] A & \splitonesub [2] B} \and
\infersplittwo [\times\mathrm{I}] 
	{\splittwo {(e_1,e_2)}{A\times B}{(p_1,p_2), l.(\letin{l_1}{\pi_1~l}{r_1},\letin{l_2}{\pi_2~l}{r_2})}}
	{ \splittwosub [1] A & \splittwosub [2] B} \and
\infersplitone [\times\mathrm{E_1}] 
	{\splitone {\pi_1~e}{A}{(\pi_1(\pi_1c),\pi_2 c),l.\pi_1~r}}{\splitonesub{A\times B}} \and
\infersplittwo [\times\mathrm{E_1}] 
	{\splittwo{\pi_1~e}{A}{p,l.\pi_1~r} }{\splittwosub{A\times B}} \and
\infersplitone [\times\mathrm{E_2}] 
	{\splitone {\pi_2~e}{B}{(\pi_2(\pi_1c),\pi_2 c),l.\pi_2~r}}{\splitonesub{A\times B}} \and
\infersplittwo [\times\mathrm{E_2}] 
	{\splittwo{\pi_2~e}{B}{p,l.\pi_2~r} }{\splittwosub{A\times B}} \and
\infersplitone [\mathrm{unit}] 
	{\splitone {v}{A}{(x,()),\_.()}}{x:A^\bbone \in \Gamma} \and
\infersplittwo [\mathrm{unit}]
	 {\splittwo{v}{A}{(),\_.x}}{x:A^\bbtwo \in \Gamma} \\
\infersplitone [\mathrm{let}] 
	{\splitone{\letin{x}{e_1}{e_2}}{B}{\cdots}}
	{\splitonesub [1] A & \splitone [\Gamma,x:A^\bbone] {e_2}{B}{c_2,l_2.r_2}} \and
\infersplittwo [\mathrm{let}] 
	{\splittwo{\letin{x}{e_1}{e_2}}{B}{(p_1,p_2), l.\letin{x}{(\letin{l_1}{\pi_1 l}{r_1})}{\letin{l_2}{\pi_2 l}{r_2}}}}
	{\splittwosub [1] A & \splittwo [\Gamma,x:A^\bbtwo] {e_2}{B}{p_2,l_2.r_2}} \and
\infersplitone [\fut\mathrm{I}] 
	{\splitone{\next~e}{\fut A}{((),p),l.r}}{\splittwosub A} \and
\infersplittwo [\fut\mathrm{E}] 
	{\splittwo{\prev~e}{A} {\pi_2~c,l.r}}{\splitonesub {\fut A}} \and
\infersplitone [+\mathrm{E}]
	{\splitone {\left( \tallif {e_1}{e_2}{e_3} \right)}{A} 
	{
		\begin{array}{l}
		\left(\tallif {\pi_1c_1}
			{\letin{y}{c_2}{(\pi_1 y, \inl(\pi_2 y))}}
			{\letin{y}{c_3}{(\pi_1 y, \inr(\pi_2 y))}}
		\right), \\
		l.\left(\caseof{\pi_1 l}{l_2.r_2}{l_3.r_3}
		\right)
		\end{array}
	}}
	{\splitonesub [1] {\rm bool} & \splitonesub [2] A & \splitonesub [3] A } \and
\infersplittwo [+\mathrm{E}]
	{\splittwo {\left( \tallif {e_1}{e_2}{e_3}\right)}{A} 
	{(p_1,p_2,p_3), 
	l.\left(\tallif{(\letin {l_1}{\pi_1 l}{r_1})}{(\letin {l_2}{\pi_2 l}{r_2})} {(\letin {l_3}{\pi_3 l}{r_3})} \right)
	}}
	{\splittwosub [1] {\rm bool} & \splittwosub [2] A & \splittwosub [3] A } \and
\end{mathpar}
\end{figure*}

\subsection {Splitting Theorems}

\subsubsection{It preserves types}

The theorem we want to show has two parts, one for the $\bbone$ code, and one for the $\bbtwo$ code.  
\begin{center}
\begin{tabular}{l}
If $\splitone {e}{A}{c,l.r}$ \\
then $\Gamma \csplit [\Gamma_1;\Gamma_2]$ \\
and $A \tsplito [A_1,A_2]$ \\
and $ \types [\Gamma_1]{c} {A_1 \times \tau}$ \\
and $ \types [\Gamma_2, p:\tau]{r} {A_2}$ 
\end{tabular}
~~~
\begin{tabular}{l}
If $\splittwo {e}{A}{p,l.r}$ \\
then $\Gamma \csplit [\Gamma_1;\Gamma_2]$ \\
and $A \tsplits A'$ \\
and $ \types [\Gamma_1]{p} {\tau}$ \\
and $ \types [\Gamma_2, p:\tau]{r} {A'}$ 
\end{tabular}
\end{center}

\subsubsection{It preserves meaning}

The theorem we want to show has two parts, one for the $\bbone$ code, and one for the $\bbtwo$ code.  
\begin{center}
\begin{tabular}{l}
If $\splitone [\emptyC]{e}{A}{c,l.r}$ \\
and $\reduceone{e}{v}$ \\
and $c \Downarrow \valprod{t}{u}$ \\
and $[u/l]r \Downarrow w$ \\
then $v \vsplito [t,w]$
\end{tabular}
~~~
\begin{tabular}{l}
If $\splittwo[\emptyC]{e}{B}{p,l.r}$ \\
and $\reducetwo{e}{v}$ \\
and $p \Downarrow u$ \\
and $[u/l]r \Downarrow w$ \\
then $v \vsplits w$
\end{tabular}
\end{center}

We proceed by cases of the term splitting judgement.

\begin{itemize}
\item {\bf Int.} Suppose that
\begin{enumerate}
\item $\splitone{i}{\rm int}{(i,()),\_.()}$
\end{enumerate}
Since reduction is a function, we also have
\begin{enumerate}
\item $\reduceone{i}{i}$
\item $\reduceone{(i,())}{\valprod {i}{()}}$
\item $\reduceone{[()/\_]()}{()}$
\end{enumerate}
And since value splitting is a function,
\begin{enumerate}
\item $i \vsplito [i,()]$
\end{enumerate}
Which is what we need to show.

\item {\bf Tuple.} Suppose that
\begin{enumerate}
\item $\splitone {(e_1,e_2)}{A\times B}
		{ \begin{array}{l}
		(\letin{y_1}{c_1}{\letin{y_2}{c_2}{((\pi_1 y_1, \pi_1 y_2),(\pi_2 y_1, \pi_2 y_2))}}), \\
		l.(\letin{l_1}{\pi_1 l}{r_1},\letin{l_2}{\pi_2 l}{r_2})
		\end{array}} $
\item $\reduceone{(e_1,e_2)}{v}$
\item $\reduce{\letin{y_1}{c_1}{\letin{y_2}{c_2}{((\pi_1 y_1, \pi_1 y_2),(\pi_2 y_1, \pi_2 y_2))}}}{\valprod tu}$
\item $\reduce{[u/l](\letin{l_1}{\pi_1 l}{r_1},\letin{l_2}{\pi_2 l}{r_2})}{w}$
\end{enumerate}
Since the splitting and evaluation rules are syntax-directed, it follows that
\begin{enumerate}
\item $\splitone {e_1}{A}{c_1,l_1.r_1}$
\item $\splitone {e_2}{B}{c_2,l_2.r_2}$
\item $v$ has the form $\valprod{v_1}{v_2}$, by inversion of tuple evaluation
\item $\reduceone{(e_1,e_2)}{\valprod{v_1}{v_2}}$
\item $\reduceone{e_1}{v_1}$
\item $\reduceone{e_2}{v_2}$
\item By inversion of evaluation, $t$ has the form $\valprod{t_1}{t_2}$  and $u$ has the form $\valprod{u_1}{u_2}$
\item $\reduceone{[\valprod{t_1}{u_1},\valprod{t_2}{u_2}/y_1,y_2]((\pi_1 y_1, \pi_1 y_2),(\pi_2 y_1, \pi_2 y_2))}{\valprod tu}$
\item $\reduceone{c_1}{\valprod {t_1}{u_1}}$, by inversion of let eval
\item $\reduceone{c_2}{\valprod {t_2}{u_2}}$, by inversion of let eval
\item By inversion of evaluation, $w$ has the form $\valprod{w_1}{w_2}$
\item $\reduce{[\valprod{u_1}{u_2}/l](\letin{l_1}{\pi_1 l}{r_1},\letin{l_2}{\pi_2 l}{r_2})}{\valprod{w_1}{w_2}}$, substituting for $u$, $w$
\item $\reduce{[u_1/l_1]r_1}{w_1}$, inversion of evaluation
\item $\reduce{[u_2/l_2]r_2}{w_2}$, inversion of evaluation
\end{enumerate}
We now have enough to invoke the inductive hypothesis,
\begin{enumerate}
\item $v_1 \vsplito [t_1,w_1]$ 
\item $v_2 \vsplito [t_2,w_2]$ 
\item $\valprod{v_1}{v_2} \vsplito [\valprod{t_1}{t_2},\valprod{w_1}{w_2}]$ 
\item $v \vsplito [t,w]$
\end{enumerate}
Which is what we need to show.
\end{itemize}

\section {Implementation}

Not much to discuss.  It exsists.  Talk about how crufty the naive transformation is.

\section{Related Work in Stage Seperation}

Our starting language, $\lambda^{12}$, is essentially a two-stage version of $\lambda^\bigcirc$ from \cite{davies96}, but beefed up with products, sums, and fixed-points.  That work motivates $\lambda^\bigcirc$ as an image of linear temporal logic under the Curry-Howard correspondence, and it makes the argument that $\lambda^\bigcirc$ is a good model for binding time analysis.  Although $\lambda^\bigcirc$ was designed with partial evaluation in mind, it turns out to be a good model for pass separation as well.  In contrast, \cite{davies01} presents $\lambda^\Box$, which is based off of branching temporal logic.  As that work shows, $\lambda^\Box$ is a good system for program generation, although we find the closed-code requirement to be more restricting than we need.

Pass seperation was introduced in \cite{jorring}.  They focused on motivating pass separation as a technique for compiler generation, and hinted at wider applicability.  Their approach was not mechanized or automated in any sense, and their examples were separated by hand.  We believe that out work represents the fulfillment of their prediction that ``the [pass separation] approach will elude full automation for some time."

\cite{hannan94} also uses a pass separation technique for generating compiler/evaluator pairs from interpretters specified as term-rewrite systems.  This heavy restriction on the form of the input prevents generalization of their style of pass separation.  [I don't know enough about term-rewrite systems yet to be able to comment further.]  Their method, while amenable to mechanization, was not actually implemented.

\cite{knoblock} represents the first attempt at fully automatic pass separation for code approaching general-purpose.  In particular, they implemented pass separation for a C-like shading language including basic arithmetic and if statements.  The main goal of their work, like ours, was to minimize recomputation. That said, their starting language was sufficiently restricted for recursive boundary types to be inexpressible.  Given that memory is at a premium in shading languages, so much of that work was also focused on minimizing the memory-footprint of the boundary type.

\bibliography{paper}
\bibliographystyle{plain}

\end{document}



