%!TEX root = paper.tex

\section{Splitting Algorithm}
\label{sec:splitting}

%\input{figures/splitting-overview}

\begin{abstrsyn}

Splitting statically transforms terms in \lang\ into a pair of
monostaged terms ($f_1$ and $f_2(-)$) that correspond to the first
stage and second stage computations in the original term. Since full
evaluation of world \bbtwo\ terms (both first stage and second stage
evaluation) only produces values in stage two, the idea of splitting
is to generate terms where evaluating $f_1$ in a first pass, and then
evaluating $f_2$ on the result of the first pass, yields the same
value as full evaluation of the original \lang\ program.

Similarly, when splitting \bbonem\ terms, first pass evaluation of
$f_1$ should produce the same value as first stage evaluation of the
original \lang\ term.  However, complexity arises because
\bbonem\ terms may also contain computations to be evaluated in the
second stage. (For example, a \bbonem\ term of type $\fut \rmint$
results in an \rmint\ value in the second stage.) Thus we also want
the second pass evaluation of $f_2$ on the outputs of $f_1$ to yield
the same value as fully evaluating all second stage computations in
the original \bbonem\ term. Finally, when splitting \bbonep\ terms,
which only produce values in the first stage, we only need to compare
the result of $f_1$ with that of the \lang\ term.

Handling the asymmetry between the results of evaluating
\bbonem\ terms (which may produce results at both the first and second
stage) and \bbtwo\ terms (which only produce second stage results) is
fundamental to the structure of the splitting algorithm presented in
this section.

\textbf{\bbtwo\ term splitting.}  For any $\coltwo e A$ which reduces
to a residual $q$ (via $\diatwo e q$) and then a final value $v$ (via
$\reduce q v$), splitting $e$ (via the new relation~$\splittwosym$)
yields a pair of monostaged terms $p$ and $l.r$. ($p$ and $l.r$
contain all first and second stage computations in $e$ respectively.)
Executing $p$ in a first pass produces $b$ (via $\reduce p b$). Then,
in the second pass, $\reduce {[b/l]r} {v'}$, where $v'$ is the same as
$v$.  Since execution of the first pass serves to generate input for
the second pass, we say $p$ is a {\em precomputation} that produces a
{\em boundary value} ($b$) for the {\em resumer} ($l.r$).

% The process of splitting \bbtwo\ terms is given by via the new
% relation~$\splittwosym$.

%masking yields the masked value $\mval i q$ (via~$\vsplito$)

\textbf{\bbonem\ term splitting.}  For any $\colmix e A$ which reduces
to $\rtab \xi v$ (via $\redonesym$) , splitting $e$ (via the new
relation~$\splitonesym$) again yields the pair of monostage terms $c$
and $l.r$. Evaluating $c$ in the first pass produces the pair $(i,b)$,
where the first element $i$ matches the first stage results contained
in $v$.  For example, in the simple case where $\rtab \xi v$ contains
entirely first stage computation, such as:
\begin{lstlisting}
2`[]`1` grnd{1+2}`
\end{lstlisting}
the first stage result is the value 3.  The second element of the pair
is used as input to the resumer in second pass evaluation as $\reduce
{[b/l]r} {v'}$, where $v'$ is the same as result produced by second
stage computations in $e$.  For example, in the case where $\rtab \xi
v$ contains entirely second stage computation, such as:
\begin{lstlisting}
2`[yhat |-> 2+3]`1` next{`2`yhat`1`}`
\end{lstlisting}
the second stage result is the value 5. Since in general residual
tables and partial values describe a mixture of both first and second
stage computations, in the subsequent subsection we define an
operation, called {\em masking}, that separates residual tables and
partial values into stage one and stage two results. We then describe
the algorithm for splitting both \bbonem\ terms ($\splitonesym$) and
\bbtwo\ terms ($\splittwosym$).


%Since $c$ ensapulates computations that produce both $e$'s first-stage
%result and also the boundary value required by the resumer, we refer
%to it as a {\em combined term}.

%Note that since $\coltwo e A$ reduces to an entirely second-stage
%residual, its first-stage computations only exist to generate input
%for second stage computations.  In contrast, since $\colmix e A$
%reduces to a multi-stage residual table and partial value, its first
%stage computations serve to both generate the value used as input for
%the second stage {\em and also} produce a first-stage value.
%Correspondingly, the precomputation $p$ resulting from splitting
%\bbtwo\ terms reduces only to a boundary value $b$ which is passed to
%the resumer; but the combined term $c$ resulting from splitting
%\bbonem\ terms reduces to a tuple containing both the first stage {\em
%  immediate result} $i$ and the boundary value $b$ (motivating the
%name {\em combined term}). 

\newpage

\subsection{Masking}
\label{sec:masking}

%Masking separates residual tables and partial values into first stage
%results and second stage residuals.

Masking (given by the relation $\vsplito$) converts a residual table $\xi$
and associated partial value $v$ into a masked value $\mval i q$.
%that explicitly separates the first stage components of 
%result from the residual.
%where $i$ is a monostage value encoding all
%first stage components of the partial value, and $q$ is a monostage
%term encoding all of the second-stage computations of the partial
%value.
The first part of the masked value, $i$, is a monostage value holding
the result of all first stage components of $v$.  The second
component, $q$, is a residual representing all second stage
computations in $\xi$ and $v$.

%(A precise definition of masking is
%given in \ref{sec:masking}.)

%Masking is not a transformation performed as part of the splitting
%algorithm.  Instead, masking defines an intuitive notion of the first
%stage and second stage ``components'' of residual tables and partial
%values, and it is used to define the requirements of what output
%splitting of \bbonem\ terms must produce.
To provide intuition about the intent of masking, consider the
following residual table and partial value:
\begin{lstlisting}
2`[yhat |-> 1+2]`1` (next{`2`yhat`1`}, injL (grnd{7}, next{`2`yhat`1`}))`
\end{lstlisting}
To construct the value $i$ representing its first-stage components,
masking first redacts all second stage (blue) parts, along with the
surrounding \texttt{next} annotations. (This redaction removes the
entire residual table since it only represents second stage
computations.) The resulting ``holes'' in the term are replaced with
unit values.
%\begin{lstlisting}
%4`##########`1` (`4`######`1`, injL (grnd{7}, `4`######`1`))`
%\end{lstlisting}
%The holes inside the term needs to be filled with something to convey the lack of information, so we use a unit values:
\begin{lstlisting}
1`((), injL (grnd{7},()))`
\end{lstlisting}
Finally, masking drops \texttt{grnd} annotations yielding:
\begin{lstlisting}
1`((), injL (7,()))`
\end{lstlisting}
To construct the residual $q$ (corresponding to second stage computations) masking first redacts all \texttt{grnd} blocks
(replacing them with unit), \texttt{next} annotations, and injection tags:
%\begin{lstlisting}
%2`[yhat |-> 1+2]`1` (`4`####`2`yhat`4`#`1`, `4`####`1` (`4`####`1`,`4`####`2`yhat`4`#`1`))`
%\end{lstlisting}
%Where the \texttt{grnd} blocks left a hole, we put in a unit values:
\begin{lstlisting}
2`[yhat |-> 1+2] (yhat, ((),yhat))`
\end{lstlisting}
Then the residual table is reified into \texttt{let} bindings,
yielding:
\begin{lstlisting}
2`let yhat = 1+2 in (yhat, ((),yhat))`
\end{lstlisting}
A precise definition of the masking relation is given in
\ref{fig:valMask}.

%Since lambdas may represent multi-stage computations, masking splits
%the body of lambdas as general world \bbonem\ terms (as described in
%\ref{sec:split-one}), and packages the resulting terms as functions in
%both the first and second stage parts of the masked value.

% Masking operates by first inducting on the entries of the residual table.  
% Being purely second-stage content, these are reified into let statements at the top of the resumer.
% Once the table is empty, masking inducts on value itself.

% Masking assigns ground values to the immediate value
% and likewise assigns references into the residual table to the resumer.
% In both cases, the alternate component is assigned to $\tup{}$, to represent trivial information.
% Note that the \texttt{grnd} and \texttt{next} annotations are erased.

% Masking distributes into tuples, injections, and rolls, since their subvalues may have content at both stages.
% However, the tags of injections and rolls are replicated only in the immediate value, 
% since they represent first-stage information.

\subsection{Term Splitting at \bbonem}
\label{sec:split-one}

We now show how to translate terms $\colmix e A$ into the form $\pipeM c l r$,
pursuant to the correctness condition given in \ref{fig:splittingSummary}.
The algorithm is specified by the $\splitonesym$ relation (\cref{fig:termSplit}), 
which proceeds recursively on the structure of~$e$.

We start with splitting the unit value, $\tup{}$.
Masking tells us that units, being trivial, only split into more units: $\tup{} \vsplito \mval {\tup{}} {\tup{}}$.
Plugging this in to the contract for splitting at \bbonem\ gives:
\[
	\text{if } \splitone {\tup {}} A c {l.r} \text{ then } \reduce c {\tup{\tup{},b}} \text{ and } [b/l]r \equiv \tup{}
\]
Although the only hard requirement for $b$, $c$, and $l.r$ is to satisfy these relations,
our actual choices are designed to do the least work possible.
In particular, we choose $b = \tup{}$; $c=\tup{\tup{},\tup{}}$; and $l.r = \_.\tup{}$.
That is, we choose a trivial boundary value.
All together, this yield the rule in \cref{fig:termSplit}.
This exercise proceeds much the same way for \texttt{grnd} blocks, 
with the purely first stage contents of the block standing in for one of the $\tup{}$s.

We next consider splitting a non-terminal, namely $\inl {e}$.
From the semantics, we already know that 
for some $\xi$ and $v$, $\diaone {e} \xi v$ and $\diaone {\inl e} \xi {\inl v}$.
Masking these outputs then gives $\rtab \xi v \vsplito \mval i q$ 
and $\rtab \xi {\inl v} \vsplito \mval {\inl i} q$.
Since splitting works from the inside out, we already know that
\[
	\splitone e A c {l.r} \text{ and } \reduce c {\tup{i,b}} \text{ and } [b/l]r \equiv q
\]
and what we need to do is find some $b'$, $c'$, and $l'.r'$ satisfying
\[
	\reduce {c'} {\tup{\inl i,b'}} \text{ and } [b'/l']r' \equiv q
\]
Once again, we take the simplest solution, which in this case is $b' = b$; $l'.r' = l.r$;
and $c' = \letin {\tup {x,y}} c {\tup{\inl x, y}}$.  
This sort of technique generalize to all constructs which don't involve control flow.

\nr{working from here down}

Splitting {\tt case} yields a combined term that executes one of the branches' combined terms based on the immediate result $y_1$ of the predicate.
The boundary value $b_i$ for this branch is injected and bundled with that of the predicate ($b_1$).   
$b_i$ is cased in the resumer to determine which branch's resumer should be executed.
{\tt case} is the only rule where splitting adds non-trivial logic is added to the precomputation.

Function introduction has a $\tup{}$ boundary value,
since functions are already fully reduced in our semantics.
However, since the body of a function may itself be multistage, splitting must continue into it.
The immediate result is a new function formed from the first-stage part of the original body.
The resumer is a new function formed out of the second-stage part of the original body.
It is the responsibility of the application site to save the precomputation of the function body
and pass it to the resumer version of the function.

Since the results of splitting \texttt{next} terms depend on the output of
splitting its world \bbtwo\ subterm,
we defer description of \texttt{next} until after describing world \bbtwo\ term splitting.

\subsection{Term Splitting at \bbtwo}

Because world \bbtwo\ terms in \lang\ reduce to monostage residuals (as opposed to partial values),
term splitting at world \bbtwo\ assumes a simpler form than the version at \bbonem\ does. 
The algorithm is specified by the $\splittwosym$ relation in \cref{fig:termSplit}.

In the terminal cases of
constants and variables, splitting generates trivial precomputations that are \texttt{()}, and resumers consisting of the original term.
For example, the integer constant \texttt{3} splits into the
precomputation \texttt{()} and resumer \texttt{\_=>3}.

More generally, for all (except \texttt{prev}) 
$n$-ary terms $e = \mathcal{C}\ttlpar e_1 \ttsemi \ldots \ttsemi e_n \ttrpar$ 
the precomputation is the tupled precomputations of $e$'s $n$ subterms:
$p=(p_1,\ldots,p_n)$.  The resumer binds each boundary value to an
argument $(l_1,\ldots,l_n)$ in a term that has the same structure
of~$e$ but where each subterm is replaced by its corresponding resumer:
$r = \mathcal{C}\ttlpar r_1 \ttsemi \ldots \ttsemi r_n \ttrpar$ .
Notably, at \texttt{case}s and functions the
precomputation of subterms is lifted out from underneath world \bbtwo\ binders.  
% TODO: should probably draw a parallel to the same behavior in dynamics

Splitting \texttt{prev} generates a precomputation that projects the immediate
result of its world \bbone\ subterm.
Since the argument to \texttt{prev} is of $\fut$ type, its immediate result reduces to $\tup{}$, justifying why it can be thrown away.
Finally, splitting \texttt{next} simply tuples up the precomputation of its
world \bbtwo\ subterm with a trivial immediate result $\tup{}$.

\subsection {Necessity of \bbonep}
\label{sec:needGround}

We have not yet given a justification for why first stage code must be
partitioned between the worlds \bbonem\ and \bbonep.
Certainly one could imagine a simpler system without all the \texttt{grnd} annotations.  
Why wouldn't this work?

In order to be correct, any code at \bbonem\ must be split with the 
pessimistic assumption that it may result in work at the second stage.
When this assumption turns out not to be true---that is, purely monostage code---then 
splitting may produce second stage code which is needlessly costly 
(an example of this will occur in \ref{sec:exampleQS}).
Detecting and optimizing this case is in general a global program analysis,
since functions can be passed around as values.  
So instead of relying on hefty analysis, 
we take the approach of adding enough structure to the input language's type system to
allow the input code to {\em prove} itself to be---in some parts---monostage.

We find that giving input with \texttt{grnd} annotations is not especially cumbersome in practice.
Yet even if a language designer desired a different implicit/explicit trade-off,
then the three-world \lang\ would still be useful as a typed intermediate representation.

\end{abstrsyn}

\input{figures/splitting-misc}
\input{figures/splitting-term}
