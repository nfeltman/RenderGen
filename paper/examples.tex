%!TEX root = paper.tex
 \section{Examples of Splitting}
\label{sec:examples}

Now we investigate the behavior of our splitting algorithm on several examples.
The split code that follows is the output of our splitting implementation; for
clarity, we have performed some minor optimizations and manually added type
annotations (including datatype declarations and constructor names), as our
algorithm does not type its output.

\subsection{Dot Product}

Our first example, \texttt{dot}, appears in \cite{knoblock96}. \texttt{dot} is a
first-order, non-recursive function---precisely the sort of code studied in
prior work on pass separation for imperative languages. \texttt{dot} takes the
dot product of two three-dimensional integer vectors, where the first two
coordinates are first-stage, and the last coordinate is second-stage. 
%
\begin{lstlisting} 
1`type vec = ^int * ^int * $`2`int`

1`dot : vec * vec -> $int
fun dot (
  (3`gr{`1`xliteralone`3`}`1`,`3`gr{`1`yliteralone`3`}`1`,next{`2`zliteralone`1`}): vec,
  (3`gr{`1`xliteraltwo`3`}`1`,`3`gr{`1`yliteraltwo`3`}`1`,next{`2`zliteraltwo`1`}): vec) = 
next{
  2`prev{`1`hold `3`gr{`1`(x1*x2) + (y1*y2)`3`}`2`} + (z1*z2)`
1`}`
\end{lstlisting}
%
The body of \texttt{dot} is a \rmint{} term at world \bbtwo\ containing a
\rmint\ computation of \texttt{(x1*x2) + (y1*y2)} which is promoted from world
\bbonep\ to world \bbtwo. We would expect the first stage of the split program
to take the first two coordinates of each vector and perform that first-stage
computation; and the second stage to take the final coordinates and the result
of the first stage, then multiply and add.

Our algorithm splits \texttt{dot} into the two functions:
%
\begin{lstlisting} 
1`fun dot1 ((x1,y1,()),(x2,y2,())) 
  = ((), (x1*x2)+(y1*y2))`

2`fun dot2 ((((),(),z1),((),(),z2)),l) 
  = l+(z1*z2)`
\end{lstlisting}
%
As expected, \texttt{dot1} returns \texttt{(x1*x2)+(y1*y2)} as the
precomputation, and \texttt{dot2} adds that precomputation to the products of
the final coordinates. This is exactly what is done in \cite{knoblock96}, except
that they write the precomputation into a mutable cache.

\subsection{Exponentiation by Squaring}

Our next example, \texttt{exp}, is a mainstay of the partial evaluation
literature (for example, in \cite{jones96}). \texttt{exp} recursively computes
$b^e$ using exponentiation by squaring, where $e$ is known at the first stage,
and $b$ is known at the second stage. 
%
\begin{lstlisting} 
1`fun exp (next{`2`b`1`} : $`2`int`1` , `3`gr{`1`e`3`}`1` : ^int) = 
  if `3`gr{`1`e == 0`3`}`1` then 
    next{`2`literalone`1`}
  else if `3`gr{`1`(e mod 2) == 0`3`}`1` then
    exp(next{`2`b*b`1`},`3`gr{`1`e/literaltwo`3`}`1`)
  else 
    next{`2`b * prev{`1`exp(next{`2`b*b`1`},`3`gr{`1`(e-1)/literaltwo`3`}`1`)`2`}`1`}`
\end{lstlisting}
%
Because \texttt{exp} is a recursive function whose conditionals test the parity
of the exponent argument, the sequence of branches taken corresponds exactly to
the binary representation of $e$. Partially evaluating \texttt{exp} with $e$
eliminates all of the conditionals, selecting and expanding the appropriate
branch in each case.

Our algorithm, on the other hand, produces:
%
\begin{lstlisting} 
datatype binnat = Zero
                | Even of binnat 
                | Odd of binnat

1`fun exp1 (b : unit, e : int) =
  if e==0 then
    ((), Zero)
  else if (e mod 2)==0 then 
    ((), Even (#2 (exp1 ((), e/2))))
  else 
    ((), Odd (#2 (exp1 ((), (e-1)/2))))`

2`fun exp2 ((b : int, e : unit), l : binnat) =
    case l of
      Zero => 1
    | Even n => exp2 ((b*b, ()), n)
    | Odd n => b * exp2 ((b*b, ()), n)`
\end{lstlisting}
%
\texttt{exp1} recursively performs parity tests on $e$, but unlike \texttt{exp},
it simply computes a data structure (a \texttt{binnat}) recording which branches
were taken. \texttt{exp2} takes $b$ and a \texttt{binnat} $l$, and uses $l$ to
determine how to compute with $b$.

Of course, the \texttt{binnat} computed by \texttt{exp1} is precisely the binary
representation of $e$! While partial evaluation realizes \texttt{exp}'s
control-flow dependency on a fixed $e$ by recursively expanding its branches in
place, we explicitly record this dependency generically over all $e$ by creating
a boundary data structure. This occurs in the $\splitonesym$ rule for
\texttt{case}, which emits a tag corresponding to the taken branch in the
precomputation, and \texttt{case}s on it (as $l_b$) in the residual.

Because splitting \texttt{exp} does not eliminate its conditionals, partial
evaluation is more useful in this case. (Notice, however, that partially
evaluating \texttt{exp2} on a \texttt{binnat} is essentially the same as 
partially evaluating \texttt{exp} on the corresponding \rmint.) Nevertheless,
splitting \texttt{exp} still demonstrates how our algorithm finds interesting
data structures latent in the structure of recursive functions.

\subsection{Quickselect}
\label {sec:exampleQS}

Let us return to the quickselect algorithm, which we discussed at
length in \ref{sec:overview}. (The code is in \ref{fig:qs-staged}.)
\texttt{qss} finds the $k$th largest element of a list $l$ by
recursively partitioning the list by its first element, then recurring
on the side containing the $k$th largest element. $l$ is first-stage and $k$ is
second-stage.

Stage-splitting \texttt{qss} produces:
%
\begin{lstlisting} 
datatype list = Empty | Cons of int * list
datatype tree = Leaf
              | Branch of int * tree * int * tree

1`fun partition ((p,l):int*list) : (int*list*list) = 
  case l of 
    Empty => (0,Empty, Empty) 
  | Cons (h,t) => 
      let val (s,left,right) = partition (p,t) in 
      if h<p 
      then (s+1,Cons(h,left),right) 
      else (s,left,Cons(h,right))

fun qs1 (l : list, k : unit) = 
  ((), case l of
    Empty => Leaf
  | Cons (h,t) => 
      Branch (
        let val (left,right,n) = partition (h,t) in
        (n, #2 (qs1 left k), h, #2 (qs1 right k))))`

2`fun qs2 (((), k : int), p : tree) = 
  case p of
    Leaf => 0
  | Branch (n,left,h,right) =>
      case compare k n of 
        LT => qs2 (((), k), left) 
      | EQ => h 
      | GT => qs2 (((), k-n-1), right)`
\end{lstlisting}
%
This is nearly identical to the cleaned-up code we presented in
\ref{fig:qs-split}, except we do not suppress the trivial inputs and outputs of 
\texttt{qs1} and \texttt{qs2}.

The function \texttt{qs1} partitions $l$, but since the comparison with $k$ (to
determine which half of $l$ to recur on) is at the second stage, it simply
recurs on \emph{both} halves, pairing up the results along with $h$ (the head of
$l$) and $n$ (the size of the left half). \texttt{qs2} takes $k$ and this tree
$p$, and uses $k$ to determine how to traverse $p$.

How does our splitting algorithm generate binary search trees and a traversal
algorithm? The $\splittwosym$ rule for \texttt{case} tuples up the
precomputations for its branches, and in the residual, selects the residual
corresponding to the appropriate branch. The tree is implicit in the structure
of the code; ordinarily, the quickselect algorithm only explores a single
branch, but the staging annotations force the entire tree to be built.

This is an instance where stage-splitting is more practical than partial
evaluation---if $l$ is large, partially evaluating \texttt{quickselect} requires
runtime generation of a huge amount of code simultaneously encoding the tree and
traversal algorithm. (Avoiding the code blowup, by not expanding some calls to
\texttt{partition}, would result in duplicating first-stage computations.)

Thus, \texttt{qs1} performs $\Theta(n \log n)$ work per call, whereas
\texttt{qs2} performs expected $\Theta(\log n)$ work.  This results in a net
speedup over standard quickselect if we perform $\Omega(n / \log n)$ queries on
the same list. 

Note that the recursive \texttt{partition} function is defined within a \texttt{gr}
annotation.  As explained in \ref{sec:needGround}, defining \texttt{partition}
at \bbonem\ would cause it to split in a way that incurs extra cost at the second stage.
In this case, that cost would be $\Theta(n)$ in the size of the input list,
enough to overpower the asymptotic speedup gained elsewhere.
With \texttt{gr} annotations, however, this can be prevented.

To run many order statistics queries $\texttt{k1},\dots,\texttt{km}$ on the same
list, use \texttt{qs1} to compute the boundary tree in the first pass, and in
the second pass, evaluate \texttt{qs2} on that tree and each rank.
\begin{lstlisting}
1`let val (_,t) = qs1 ([7,4,2,5,9,...,3],())
in `2`(qs2 (((),k1),1`t`),...,qs2 (((),km),1`t`))`
\end{lstlisting}
%
In the appendix provided as supplemental material, we extend this example by
splitting \texttt{map (qss l)} over an arbitrary list of ranks \texttt{ks}. We also
provide an example of a program whose boundary data structure is a trie.

\subsection {Composing Graphics Pipeline Programs}

A major motivation for staged programming in computer graphics is to express
logic related to a single application concern conveniently as a single staged
function (referred to as a pipeline program)\,\cite{Foley:2011}, rather than
as a collection of functions distributed across different GPU pipeline stages.  

Pipeline programs can be directly expressed as staged functions in \lang\, with splitting 
used to generate monostaged code required for each GPU pipeline stage
(as well as the plumbing to communicate values between the stages).
Composition of such functions in \lang\ allows more complex pipelines to be expressed in a modular programming style
that is becoming increasingly desirable in graphics shading languages\,\cite{Foley:2011,He:2014}. A simple example of composition of staged functions (representing GPU pipeline programs) is given below.  The code defines a pipeline as a staged function that takes an object definition in the first stage (\texttt{obj}) and a pixel coordinate (\texttt{xy}) in the second stage and emits the color of the object at the specified pixel. The code defines a pipeline combinator \texttt{shade} which modulates the results of the object's reflectance at a given pixel with the results of a function that computes the object's albedo (surface color).  
%
\begin{lstlisting} 
1`datatype object = ...`
2`atsignnext{
  type coord = int * int
  type color = int
}`
1`type pipeline = object * $`2`coord`1` -> `$2`color`

1`fun shade (refl : pipeline,
           albedo : pipeline) : pipeline =
  (fn (obj : object, next{2`xy`} : $2`coord`) =>
       2`prev{`1`refl   (obj,next{2`xy`})`2`}
     * prev{`1`albedo (obj,next{2`xy`})`2`}`)
\end{lstlisting}
%
Most pipeline programming models either force the user to manually split their programs (which harms modularity).
Others, such as \cite{Foley:2011}, offer only a few multistage types (namely, first order functions) and give 
combinators like \texttt{par} as language features.
In contrast, \lang\ is powerful enough to define those combinators in-language.

