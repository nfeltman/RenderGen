%!TEX root = paper.tex
 \section{Examples of Splitting}
\label{sec:examples}

Now we investigate the behavior of our splitting algorithm on several examples.
The split code that follows is the output of our splitting implementation; for
clarity, we have performed some minor optimizations and manually added type
annotations (including datatype declarations and constructor names), as our
algorithm does not type its output.

\subsection{Dot Product}

Our first example, \texttt{dot}, appears in \cite{knoblock96}. \texttt{dot} is a
first-order, non-recursive function---precisely the sort of code studied in
prior work on pass separation for imperative languages. \texttt{dot} takes the
dot product of two three-dimensional vectors, where the first two
coordinates are first-stage, and the last coordinate is second-stage. 
%
\begin{lstlisting} 
1`type vec = ^int * ^int * $`2`int`
1`// dot : vec * vec -> $int
fun dot ((3`gr{`1`xliteralone`3`}`1`,`3`gr{`1`yliteralone`3`}`1`,next{`2`zliteralone`1`}),(3`gr{`1`xliteraltwo`3`}`1`,`3`gr{`1`yliteraltwo`3`}`1`,next{`2`zliteraltwo`1`})) = 
  next{ 2`prev{`1`hold `3`gr{`1`(x1*x2) + (y1*y2)`3`}`2`} + (z1*z2)` 1`}`
\end{lstlisting}
%
The body of \texttt{dot} is an \rmint\ term at world \bbtwo\ containing an
\rmint\ computation of \texttt{(x1*x2) + (y1*y2)} which is promoted from world
\bbonep\ to world \bbtwo. We would expect the first stage of the split program
to take the first two coordinates of each vector and perform that first-stage
computation; and the second stage to take the final coordinates and the result
of the first stage, then multiply and add.
Our algorithm splits \texttt{dot} into the two functions:
\begin{lstlisting} 
1`fun dot1 ((x1,y1,()), (x2,y2,()))    = ((), (x1*x2)+(y1*y2))`
2`fun dot2 ((((),(),z1),((),(),z2)),l) = l+(z1*z2)`
\end{lstlisting}
%
As expected, \texttt{dot1} returns \texttt{(x1*x2)+(y1*y2)} as the
precomputation, and \texttt{dot2} adds that precomputation to the products of
the final coordinates. This is exactly what is done in \cite{knoblock96}, though
they write the precomputation into a mutable cache.

\subsection{Exponentiation by Squaring}

Our next example, \texttt{exp}, is a mainstay of the partial evaluation
literature (for example, in \cite{jones96}). \texttt{exp} recursively computes
$b^e$ using exponentiation by squaring, where $e$ is known at the first stage,
and $b$ is known at the second stage. 
%
\begin{lstlisting} 
1`fun exp (next{`2`b`1`} : $`2`int`1` , `3`gr{`1`e`3`}`1` : ^int) = 
 if      `3`gr{`1`e == 0`3`}`1`       then next{`2`literalone`1`}
 else if `3`gr{`1`e mod 2 == 0`3`}`1` then exp(next{`2`b*b`1`},`3`gr{`1`e/literaltwo`3`}`1`)
 else                     next{`2`b*prev{`1`exp(next{`2`b*b`1`},`3`gr{`1`(e-1)/literaltwo`3`}`1`)`2`}`1`}`
\end{lstlisting}
%
Because \texttt{exp} is a recursive function whose conditionals test the parity
of the exponent argument, the sequence of branches taken corresponds exactly to
the binary representation of $e$. Partially evaluating \texttt{exp} with $e$
eliminates all of the conditionals, selecting and expanding the appropriate
branch in each case.

Our algorithm, on the other hand, produces:
%
\begin{lstlisting} 
datatype binnat = Zero
                | Even of binnat 
                | Odd of binnat

1`fun exp1 (b : unit, e : int) =
  if      e == 0       then ((), Zero)
  else if e mod 2 == 0 then ((), Even (#2 (exp1 ((), e/2))))
  else                      ((), Odd  (#2 (exp1 ((), (e-1)/2))))`

2`fun exp2 ((b : int, e : unit), l : binnat) =
    case l of
      Zero => 1
    | Even n => exp2 ((b*b, ()), n)
    | Odd n => b * exp2 ((b*b, ()), n)`
\end{lstlisting}
%
\texttt{exp1} recursively performs parity tests on $e$, but unlike \texttt{exp},
it simply computes a data structure (a \texttt{binnat}) recording which branches
were taken. \texttt{exp2} takes $b$ and a \texttt{binnat} $l$, and uses $l$ to
determine how to compute with $b$.

Of course, the \texttt{binnat} computed by \texttt{exp1} is precisely the binary
representation of $e$! While partial evaluation realizes \texttt{exp}'s
control-flow dependency on a fixed $e$ by recursively expanding its branches in
place, we explicitly record this dependency generically over all $e$ by creating
a boundary data structure. This occurs in the $\splitonesym$ rule for
\texttt{case}, which emits a tag corresponding to the taken branch in the
precomputation, and \texttt{case}s on it (as $l_b$) in the residual.

Because splitting \texttt{exp} does not eliminate its conditionals, partial
evaluation is more useful in this case. (Notice, however, that partially
evaluating \texttt{exp2} on a \texttt{binnat} is essentially the same as 
partially evaluating \texttt{exp} on the corresponding \rmint.) Nevertheless,
splitting \texttt{exp} still demonstrates how our algorithm finds interesting
data structures latent in the structure of recursive functions.

\subsection{Quickselect}
\label {sec:exampleQS}

Let us return to the quickselect algorithm, which we discussed at
length in \ref{sec:overview}. (The code is in \ref{fig:qs-staged}.)
\texttt{qss} finds the $k$th largest element of a list $l$ by
recursively partitioning the list by its first element, then recurring
on the side containing the $k$th largest element. $l$ is first-stage and $k$ is
second-stage.

Stage-splitting \texttt{qss} produces:
%
\begin{lstlisting} 
datatype tree = Leaf | Branch of int * tree * int * tree
1`datatype list = Empty | Cons of int * list

fun part ((p,l):int*list) : (int*list*list) = 
  case l of Empty => (0,Empty, Empty) 
  | Cons (h,t) => 
      let val (n,left,right) = part (p,t) in 
      if h<p then (n+1,Cons(h,left),right) 
             else (n,left,Cons(h,right))

fun qs1 (l : list, k : unit) = 
  ((), case l of Empty => Leaf
       | Cons (h,t) => Branch (
           let val (n,left,right) = part (h,t) in
           (n, #2 (qs1 left k), h, #2 (qs1 right k))))`

2`fun qs2 (((), k : int), p : tree) = 
  case p of Leaf => 0
  | Branch (n,left,h,right) =>
      case compare k n of 
        LT => qs2 (((), k), left) 
      | EQ => h 
      | GT => qs2 (((), k-n-1), right)`
\end{lstlisting}
%
This is nearly identical to the cleaned-up code we presented in
\ref{fig:qs-split}, except we do not suppress the trivial inputs and outputs of 
\texttt{qs1} and \texttt{qs2}.

The function \texttt{qs1} partitions $l$, but since the comparison with $k$ (to
determine which half of $l$ to recur on) is at the second stage, it simply
recurs on \emph{both} halves, pairing up the results along with $h$ (the head of
$l$) and $n$ (the size of the left half). \texttt{qs2} takes $k$ and this tree
$p$, and uses $k$ to determine how to traverse $p$.

How does our splitting algorithm generate binary search trees and a traversal
algorithm? The $\splittwosym$ rule for \texttt{case} tuples up the
precomputations for its branches, and in the residual, selects the residual
corresponding to the appropriate branch. The tree is implicit in the structure
of the code; ordinarily, the quickselect algorithm only explores a single
branch, but the staging annotations force the entire tree to be built.

This is an instance where stage-splitting is more practical than partial
evaluation; if $l$ is large, partially evaluating \texttt{quickselect} requires
runtime generation of a huge amount of code simultaneously encoding the tree and
traversal algorithm. (Avoiding the code blowup, by not expanding some calls to
\texttt{part}, would result in duplicating first-stage computations.)

Note that the recursive \texttt{part} function is defined within a \texttt{gr}
annotation.  As explained in \ref{sec:needGround}, defining \texttt{part}
at \bbonem\ would cause it to split in a way that incurs extra cost at the second stage.
In this case, that cost would be $\Theta(n)$ in the size of the input list,
enough to overpower the asymptotic speedup gained elsewhere.
With \texttt{gr} annotations, however, this can be prevented.

As discussed in \ref{sec:overview}, \texttt{qs1} performs $\Theta(n \log n)$ expected work per call, 
whereas \texttt{qs2} performs $\Theta(\log n)$ expected work.  
This results in a net speedup over standard quickselect if we 
perform many (specifically, $\omega(\log n)$) queries on
the same list---precisely the topic of our next example. 

\subsection {Mixed-Stage Map Combinator}

As a lambda calculus, one of the strengths of \lang\ is that it
can express combinators as higher order functions.
In this example, we consider just such a combinator: \texttt{tmap},
which turns a function of type
$\curr \mathtt{list} * \fut \mathtt{int} \to \fut \mathtt{int}$
into one of type
$\curr \mathtt{list} * \fut \mathtt{list2} \to \fut \mathtt{list2}$,
by mapping over the second argument.\footnote{
\lang\ doesn't have a way to ``share'' datatype declarations between stages, 
so we define \texttt{list2} to be a list of integers at the second stage.}
\begin{lstlisting} 
1`atsignnext{`2` datatype list2 = Empty2 | Cons2 of int * list2 `1`}

fun tmap (f : ^list * $`2`int`1` -> $`2`int`1`) (l : ^list, q : $`2`listliteraltwo`1`) = 
  next{`2` let fun m Empty2       = Empty2
              | m (Cons2(h,t)) = Cons2(prev{`1`f (l,next{`2`h`1`})`2`}, m t)
        in m prev{`1`q`2`}`1`}
val mapqss = tmap qss`
\end{lstlisting}
Importantly, \texttt{tmap f} performs the first-stage part of \texttt{f} once
and second-stage part of \texttt{f} many times.
This was discussed in the context of partial evaluation in \ref{sec:topLevel}, 
but it is especially clear when we look at the output of splitting:
\begin{lstlisting} 
1`fun tmap1 f = (fn (l,()) => ((),#2 (f (l,()))),())
val (mapqss1,()) = tmap1 qs1

`2`datatype list2 = Empty2 | Cons2 of int * list2
fun tmap2 (f,()) ((l,q), p : tree) =
  let fun m Empty2 = Empty2
        | m (Cons2(h,t)) = Cons2(f ((l,h), p), m t) 
  in m q
val mapqss2 = tmap2 (qs2, ())`
\end{lstlisting}
%
Indeed, observe that the argument \texttt{f} (for which \texttt{qs1} is later substituted) of \texttt{tmap1} is called only once,
whereas the corresponding argument \texttt{f} (for which \texttt{qs2} is later substituted) in \texttt{tmap2} is evaluated once 
per element in the query list \texttt{q}.

\subsection{Composing Graphics Pipeline Programs}
\label{sec:graphics}

Composable staged programs are particularly important the domain of real-time graphics.
This need arises because modern graphics architectures actually
\emph{require} that graphics computations be structured as a pipeline
of stages which perform increasingly fine-grained computations (e.g.,
per-object, per-screen region, per-pixel), where computations in later
stages use the results of an earlier
stage multiple times\,\cite{OpenGL4Spec}.

The standard way to program these graphics pipelines is 
to define one program (usually called a \emph{shader}) per stage.
In other words, the programmer is expected to write their 
multi-stage programs in an already split form.
This requirement results in complex code where invariants must hold across different stages and
local changes to the logic of one stage may require changes to that of upstream stages. 
This harms composition and modularity.  
Graphics researchers therefore have suggested using mechanisms like our stages
\,\cite{Proudfoot:2001,Foley:2011,He:2014} to express graphics program logic,
including representing entire pipeline as a single multi-stage function\,\cite{Foley:2011}.

As a language, \lang\ is well suited for specification of such functions,
and we give a simple example below.
In it, we consider a graphics pipeline program to be a
staged function that takes an object definition in the first stage (\texttt{object})
and a pixel coordinate in the second stage ($\fut$\texttt{coord}), 
and emits the color of the object at the specified pixel ($\fut$\texttt{color}). 
Given two such multistage functions, we then define a combinator
that multiplies their results pointwise in the second 
stage.\footnote{Although this is a general pointwise multiplication combinator, 
the variable names suggest a possible interpretation of the inputs and output:
the first input function calculates the albedo of an object (a measure of how much light it reflects),
the second input calculates the object's incoming light,
and so the output (the product of these terms) is a function that calculates 
the outgoing light from an object.}
%
\begin{lstlisting} 
1`datatype object = ...`
2`atsignnext{  type coord = int * int
        type color = ...        }`
1`type pipeline = object * $`2`coord`1` -> `$2`color`

1`fun shade (refl : pipeline, albedo : pipeline) : pipeline =
  fn (obj : object, next{2`xy`} : $2`coord`) =>
       2`prev{`1`refl (obj,next{2`xy`})`2`} * prev{`1`albedo (obj,next{2`xy`})`2`}`
\end{lstlisting}
%
Prior work\,\cite{Foley:2011} lacks the ability to define such combinators,
because it does not support higher order functions.