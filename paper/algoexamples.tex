%!TEX root = paper.tex
 \section{Examples of Splitting}
\label{sec:examples}

Now we investigate the behavior of our splitting algorithm on several examples.
While our splitting algorithm does not yield typed output terms, in practice we
find it possible to assign types to many such outputs. In the examples that
follow, for the sake of readability, we have manually added type annotations
(including datatype declarations and constructor names) and performed some minor
optimizations.

\subsection{Dot Product}

Our first example takes the dot product of two three-dimensional integer
vectors, where the first two coordinates are first-stage, and the last
coordinate is second-stage. \texttt{dot} is a first-order, non-recursive
function---precisely the sort of code studied in prior work on pass separation
for imperative languages. (A very similar example appears in \cite{knoblock96}.)
%
\begin{lstlisting} 
1`type vec = ^int * ^int * $`2`int`

1`//dot : vec * vec -> $int
fun dot (
  (3`grnd{`1`x1`3`}`1`,`3`grnd{`1`y1`3`}`1`,next{`2`z1`1`}): vec,
  (3`grnd{`1`x2`3`}`1`,`3`grnd{`1`y2`3`}`1`,next{`2`z2`1`}): vec) = 
next{
  2`prev{`1`hold `3`grnd{`1`(x1*x2) + (y1*y2)`3`}`2`} + (z1*z2)`
1`}`
\end{lstlisting}
%
The body of \texttt{dot} is a \rmint{} term at world \bbtwo\ containing a
\rmint\ computation of \texttt{(x1*x2) + (y1*y2)} which is promoted from world
\bbonep\ to world \bbtwo. We would expect the first stage of the split program
to take the first two coordinates of each vector and perform that first-stage
computation; and the second stage to take the final coordinates and the result
of the first stage, then multiply and add.

Our algorithm splits \texttt{dot} into the two functions:
%
\begin{lstlisting} 
1`fun dot1 ((x1,y1,()),(x2,y2,())) 
  = ((), (x1*x2)+(y1*y2))`

2`fun dot2 ((((),(),z1),((),(),z2)),l) 
  = l+(z1*z2)`
\end{lstlisting}
%
As expected, \texttt{dot1} returns \texttt{(x1*x2)+(y1*y2)} as the
precomputation, and \texttt{dot2} adds that precomputation to the products of
the final coordinates. This is exactly what is done in \cite{knoblock96}, except
that they write the precomputation into a mutable cache.

\subsection{Exponentiation by Squaring}

Our next example computes $b^e$ using exponentiation by squaring, where $e$ is
known at the first stage, and $b$ is known at the second stage. This is a common
example in the partial evaluation literature (for example, in \cite{jones96}).
%
\begin{lstlisting} 
1`fun exp (next{`2`b`1`} : $`2`int`1` , `3`grnd{`1`e`3`}`1` : ^int) = 
  if `3`grnd{`1`e == 0`3`}`1` then 
    next{`2`literalone`1`}
  else if `3`grnd {`1`(e mod 2) == 0`3`}`1` then
    exp(next{`2`b*b`1`},`3`grnd{`1`e/literaltwo`3`}`1`)
  else 
    next{`2`b * prev{`1`exp(next{`2`b*b`1`},`3`grnd{`1`(e-1)/literaltwo`3`}`1`)`2`}`1`}`
\end{lstlisting}
%
Because \texttt{exp} is a recursive function whose conditionals test the parity
of the exponent argument, the sequence of branches taken corresponds exactly to
the binary representation of $e$. Partially evaluating \texttt{exp} with $e$
unrolls all of the conditionals, selecting and expanding the appropriate branch
in each case.

Our algorithm, on the other hand, produces:
%
\begin{lstlisting} 
datatype binnat = Zero | Even of nat | Odd of nat

1`fun exp1 (b : unit, e : int) =
  if e==0 then
    ((), Zero)
  else if (e mod 2)==0 then 
    ((), Even (#2 (exp1 ((), e/2))))
  else 
    ((), Odd (#2 (exp1 ((), (e-1)/2))))`

2`fun exp2 ((b : int, e : unit), l : binnat) =
    case l of
      Zero => 1
    | Even n => exp2 ((b*b, ()), n)
    | Odd n => b * exp2 ((b*b, ()), n)`
\end{lstlisting}
%
\texttt{exp1} recursively performs parity tests on $e$, but unlike \texttt{exp},
it simply computes a data structure (a \texttt{binnat}) recording which branches
were taken. \texttt{exp2} takes $b$ and a \texttt{binnat} $l$, and uses $l$ to
determine how to compute with $b$.

Of course, the \texttt{binnat} computed by \texttt{exp1} is precisely the binary
representation of $e$! While partial evaluation realizes \texttt{exp}'s
control-flow dependency on $e$ by recursively expanding its branches in place,
we realize this dependency explicitly in the boundary data structure. This
occurs in the $\splitonesym$ rule for \texttt{case}, which emits a tag
corresponding to the taken branch in the precomputation, and \texttt{case}s on
it (as $l_b$) in the residual.

While this example is not useful in practice, it still demonstrates the way our
splitting algorithm finds interesting data structures latent in the structure
of recursive functions. A more useful example follows.

%Partially evaluating this code is typically more useful, as that would
%eliminate branches in the residual, and exponents tend to be small enough to
%keep code size reasonable.

\subsection{Quickselect}
\label {sec:exampleQS}

Let us return to the \texttt{quickselect} algorithm, which we discussed at
length in \ref{sec:example}. (The code is in \ref{fig:qs-staged}.)
\texttt{quickselect} finds the $k$th largest element of a list $l$ by
recursively \texttt{partition}ing the list by its first element, then recurring
on the side containing the $k$th largest element. $l$ is first-stage and $k$ is
second-stage.

Stage-splitting \texttt{quickselect} produces:
%
\begin{lstlisting} 
datatype list = Empty | Cons of int * list
datatype tree = Leaf
              | Branch of int * tree * int * tree

1`fun partition ((p,l):int*list) : (int*list*list) = 
  case l of 
    Empty => (0,Empty, Empty) 
  | Cons (h,t) => 
      let val (s,left,right) = partition (p,t) in 
      if h<p 
      then (s+1,Cons(h,left),right) 
      else (s,left,Cons(h,right))

fun qs1 (l : list, k : unit) = ((), 
  case l of
    Empty => Leaf
  | Cons (h,t) => 
      Branch (
        let (left,right,n) = partition (h,t) in
        (n, #2 (qs1 left k), h, #2 (qs1 right k))
      )
  )`

2`fun qs2 (((), k : int), p : tree) = 
  case p of
    Leaf => 0
  | Branch (n,left,h,right) =>
      case compare (k, n) of 
        LT => qs2 (((), k), left) 
      | EQ => h 
      | GT => qs2 (((), k-n-1), right)`
\end{lstlisting}
%
\texttt{qs1} \texttt{partition}s $l$, but since the comparison with $k$ (to
determine which half of $l$ to recur on) is at the second stage, it simply
recurs on \emph{both} halves, pairing up the results along with $h$ (the head of
$l$) and $n$ (the size of the left half). \texttt{qs2} takes $k$ and this tree
$p$, and uses $k$ to determine how to traverse $p$.

How does our splitting algorithm generate binary search trees and a traversal
algorithm? The $\splittwosym$ rule for \texttt{case} tuples up the
precomputations for its branches, and in the residual, selects the residual
corresponding to the appropriate branch. The tree is implicit in the structure
of the code; ordinarily, the quickselect algorithm only explores a single
branch, but the staging annotations force the entire tree to be built.

This is an instance where stage-splitting is more practical than partial
evaluation---if $l$ is large, partially evaluating \texttt{quickselect} would
yield a huge code blowup, essentially encoding the tree as code. (Or else it
would not expand the calls to \texttt{partition}, in which case first-stage
computations would be duplicated.)

Thus, \texttt{qs1} performs $\Theta(n \log n)$ work per call, whereas
\texttt{qs2} performs expected $\Theta(\log n)$ work.  This results in a net
speedup over standard quickselect if we perform $\Omega(n / \log n)$ queries on
the same list. 

Note that the recursive \texttt{partition} function is defined within a \texttt{grnd}
annotation.  As explained in \ref{sec:needGround}, defining \texttt{partition}
at \bbonem\ would cause it to split in a way that incurs extra cost at the second stage.
In this case, that cost would be $\Theta(n)$ in the size of the input list,
enough to overpower the asymptotic speedup gained elsewhere.
With \texttt{grnd} annotations, however, this can be prevented.

\subsection {Mixed Map Combinator}

In the previous example, we suggested that staged quickselect could be applied to one list and many queries.
In this example, we implement that behavior by passing \texttt{qs} to a higher order function.
Such a combinator, which operates over both stages at once, is possible because of the generality of \lang's abstractions.

Note that we have to define a datatype for integer lists at the second stage, which is given by \texttt{list2}.
\begin{lstlisting} 
1`atsignnext{`2`
  datatype list2 = Empty2 | Cons2 of int * list2
`1`}

type qsType = ^list*$`2`int`1`->$`2`int`1`

// map : qsType -> ^list * $`2`list2`1` -> $`2`list2`1`
fun map f (l, q) = 
next {`2`
  let 
  fun m Empty2 = Empty2
    | m (Cons2(h,t)) = Cons2(prev{`1`f (l,next{`2`h`1`})`2`}, m t)
  in m prev{`1`q`2`}
`1`}

val qsMany = map qs`
\end{lstlisting}
The \texttt{map} function splits into the following two functions:
\begin{lstlisting} 
2`datatype list2 = Empty2 | Cons2 of int * list2

`1`fun map1 f = (fn (l,()) => ((), #2 (f (l, ()))), ())
`2`fun map2 (f,()) ((l,q), p : tree) =
  let 
  fun m Empty2 = Empty2
    | m (Cons2(h,t)) = Cons2(f ((l,h), p), m t) 
  in m q`
\end{lstlisting}

As desired, \texttt{qs1} (which is passed to \texttt{map1} via variable \texttt{f}) is executed only once per invocation of \texttt{map},
whereas \texttt{qs2} (which is passed to \texttt{map2} via variable \texttt{f}) is executed once per query.

\subsection {Pipeline Programming}

Due to its generality, \lang\ could make a good basis for modular programming of pipeline systems like those of modern graphics architectures.
For instance, a pipeline program which takes input of type $A$ at the first stage and produces an output of type $B$ at the second stage 
would simply be encoded by a function of type $A \to \fut B$.
From here, we could define combinators to combine pipelines, such as,
\begin{lstlisting} 
1`fun par (pipe1 : A -> $`2`B`1`, pipe2 : C -> $`2`D`1`) 
         (a : A, c : C) = 
  next{`2`(prev{`1`pipe1 a`2`}, prev{`1`pipe2 c`2`}`1`}
\end{lstlisting}
which takes two pipelines makes a pipeline on the products of their inputs and outputs.

Most pipeline programming models either force the user to manually split their programs (which harms modularity).
Others, such as \cite{Foley:2011}, offer only a few multistage types (namely, first order functions) and give 
combinators like \texttt{par} as language features.
In contrast, \lang\ is powerful enough to define those combinators in-language.