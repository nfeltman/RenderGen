%!TEX root = paper.tex

\section{Related Work}
\label{sec:related}

Frequency reduction and precomputation are common techniques for both
designing algorithms and performing compiler
optimizations~\cite{JS86-staging}.
%
The idea behind precomputation is to identify computations that can be
performed earlier (e.g., at compile time) if their inputs are
available statically and perform them at that earlier time. Dynamic
algorithms, partial evaluation, and incremental computation are all
examples of precomputation techniques.
%
The idea behind frequency
reduction is to identify computations that are performed multiple
times and pull them ahead so that they can be performed once and used
later as needed.  Dynamic programming, loop hoisting, and pass
separation (presented here) are all examples of frequency reduction
techniques.

\paragraph{Precomputation techniques.}
Perhaps one of the most studied examples of precomputation is
partial evaluation.  Partial evaluation distinguishes between static
(compile-time) and dynamic (runtime) stages. Given a program and values 
for all static inputs, partial evaluation generates a specialized program by
performing computations that depend only on the static
inputs~\cite{jones96}.  The roots of partial evaluation go back to
Kleene, who in 1952 proved that a multivariate program (function)
$f(x_1, \ldots, x_{n+m})$ can be specialized for any given $a_1 \ldots
a_m$ to obtain another program $g(x_1, \ldots, x_n)$ such that $f(a_1,
\ldots, a_m, y_1, \ldots, y_n) = g(y_1, \ldots, y_n)$~\cite{Kleene52}.
%Kleene did not intend to improve efficiency, however.  Lombardi
%appears to be the first person to use the term in his work on
%incremental computation~\cite{Lombardi67}.
There has since been an enormous body of work on partial evaluation,
and we refer the reader to the book by Jones, Gommard, and Sestoft for
a comprehensive discussion of work until the early 90's~\cite{JGS93}.

Early approaches to partial evaluation can be viewed as operating in
two stages: binding time analysis and program specialization.  For a
multivariate program with clearly marked static and dynamic arguments,
binding-time analysis identifies all the parts of the program that can
be computed by the knowledge of static arguments. Using this
information and the values of static arguments, program specialization
specializes the original program to a partially-evaluated one that
operates on many different dynamic arguments.  This approach has been
applied to construct partial evaluators for a number of languages such
as Scheme~\cite{OB91-Similix,Consel88-Schism}, although for the most
part restricted to first-order subsets.

Experience with binding time analysis showed that it can be difficult
to control, leading to programs whose performance was difficult to
predict. This led to investigations based on type systems for making
the stage of computations explicit in
programs~\cite{GJ91-lambda,NN92-twolevel} and writing ``metaprograms''
that, when evaluated at a stage, yield a new program to be evaluated
at the next stage.
%J{\o}rring and
%Scherlis describe the idea of meta programming as one of the three possible approaches to
%taking advantage of staging~\cite{JS86-staging}.
Davies~\cite{davies96} presented a
logical construction of binding-time type systems by deriving a type
system via the Curry-Howard correspondence applied to temporal logic.
Davies and Pfenning proposed a new type system for staged computation
based on a particular fragment of modal logic~\cite{DP01-modal}. The
work on MetaML extended type-based techniques to a full-scale language
by developing a statically typed programming language based on ML that
enables the programmer to express programs with multiple
stages~\cite{Taha97,taha-thesis-99}.  MetaML's type system is similar
to Davies~\cite{davies96} but extends it in several important ways.
Nanevksi and Pfenning further extended the these techniques by
allowing free variables to occur within staged
computations~\cite{NP05-nn}.

The type-system of \lang\ is closely related to this later line of
work on metaprogramming and staged computation.  The specific
extension to the typed lambda calculus that we use here is based on
the $\fut$ modality of Davies~\cite{DP01-modal}.  Our types differ in
small ways to account for the specific evaluation model that we
propose; however, the key difference between our work and this prior
work is that we instead focus on the problem of splitting.
%we developed an algorithm
%for splitting terms in the proposed staged language.
%the staging or the splitting algorithm, which given a two-staged
%program splits it into two stages that can be executed sequentially,
%passing information from the earlier to the latter stage by
%construction.
%
%Unlike partial evaluation, splitting requires no knowledge of program
%argument values, only their staging.
%Using just the staging
%information, the splitting algorithm can generate programs that can
%perform non-trivial transformations on the code automatically.  As our
%examples illustrate such staging transformations can improve the
%efficiency of the source program, sometimes in asymptotically
%significant ways.


Another class of precomputation techniques is incremental computation,
where a program can efficiently respond to small changes in its inputs
by only recomputing parts of the computation affected by the
changes~\cite{DemersReTe81,PughTe89,RamalingamRe93,AcarBlBlHaTa09}.
Thus, incremental computation can be thought of as a staged model,
where changes in inputs come over a period of time in stages.
%
However, unlike splitting, incremental computation does not require
fixing any of its inputs and, in the general case, allows for all
program inputs to change. Thus, the benefits of incremental
computation depends on what changes to inputs are made. For example,
while it is possible to apply incremental computation to our
quickselect example (\ref{sec:overview}), techniques would unfold the
quickselect function based on the demanded ranks, potentially
incurring linear time cost at each step of the algorithm (as opposed
to logarithmic result produced by splitting). Unlike the results of
splitting, such an implementation would also maintain sophisticated
data structures dynamically at run time to track what computations
must be performed.

\paragraph{Stage Splitting.}

Algorithms for stage splitting have appeared in the literature under the names 
{\em pass separation}, {\em binding time separation}, {\em data specialization}, 
{\em program bifurcation}, and as various unnamed compiler passes.

In the partial evaluation community, stage splitting has been investigated 
mostly as a preprocessing step for self-applied partial evaluators. 
For instance,~\cite{Mogensen89a} and~\cite{DeNiel91} defined splitting algorithms
for programs consisting of a set of first-order recursive functions.
Those works use a grammar-based binding-time specification that's 
relatively complicated compared to the type system of \lang.
This complication mostly arises from the decision to start with an untyped language:
one must first find a way to say ``argument $x$ will be a tuple'' before one can say
``the first component of $x$ is available at the second stage,'' 
and so much of their binding-time specification just amounts to a type system.
Formulations more like \lang\ (namely the two-level system of~\cite{NN92-twolevel}) were known at the time,
but~\cite{Mogensen89a} found these unsatisfactory because they could not describe data structures with mixed stage.
Davies essentially solved this limitation in~\cite{davies96} (pending our further tweak with $\curr$),
and the question of stage splitting as we have presented it has been ripe ever since.
Binding time analysis for a static higher-order language was visited in \cite{Mogensen89b},
but the implications for splitting were not explicitly considered.

Splitting algorithms have also been a topic of interest in programming
systems for computer graphics, where to achieve high performance,
programs are typically manually separated into components by frequency
of execution. (Indeed, separation into a pre-determined set of
per-frequency modules: e.g., per frame, per pixel, per triangle, is
\emph{required} by the execution model of modern graphics
architectures.)  The software engineering challenges of modifying
graphics programs written in a split form have spurred interest in
deferring the task of splitting to a compiler\,\cite{Foley:2011}.
While early work sought to determine the staging of programs
automatically via binding time analysis\,\cite{knoblock96}, more
recent designs, like \lang, require explicit staging by the
programmer\,\cite{Proudfoot:2001,Foley:2011,He:2014}.  However,
splitting algorithms developed in all of these efforts have been
limited to simple, imperative graphics shading languages that lack
simple features such as functions.



%% Online and offline.  Both approaches have their merits but online
%% partial evaluation can be too slow and if stopped lead to an
%% inefficient residual.  Offline partial evaluation is faster.  Also,
%% full self-application has only been achieved by offline partial
%% evaluation.


%% Weaknesses of partial evaluation include~\cite{GJ05}:
%% 1) speedups linear in the subject programs run time (excuse moi?)

%% 2) good speedups require close knowledge of the program and some
%% binding-time improvements.

%% 3) result of specialization can be hard to predict as it can lead to
%% slowdown, code explosion, and non-termination at specialization time.


%% \paragraph{Meta programming}

%% What happens when we try to write quickselect as a meta program? 


%% \ur{Umut: I am starting a paragraph on termination.  we might need to expand
%%   on this.}
%% \paragraph{Termination.}

%% There are two reasons for non-termination: infinite loops and
%% execution of errorneous code.  The splitting algorithm can cause the
%% first.  But can it also cause the second? (If all computations really
%% depend on stage 1 code, then would it be possible to for example see
%% something like 1/0?  i think so but only if there is a meta dependency
%% between the data in two stages?)


%% Is in possible for the residual program to not terminate (unexpectedly)? 

%% Termination analysis in partial evaluation: see paper~\cite{AH96,GJ05}.

%% Based on Glenstrup and Jones (pp. 1154, 1155), a partial evaluator
%% should be complete. That is every computation depending on the static
%% input should be reduced to a value (during specialization).
%% Unfortunately, completeness is difficult to achieve if we also require
%% that the specialization is always terminating.

%% One problem with their description is that it is not clear which
%% termination problem they are talking about termination of the residual
%% or the specializer. 

%% Nontermination during specialization: 
%% consider code

%% double x = doubleplus (x,0) 

%% doubleplus (u,v) = if u <= 0 then v else doubleplus (u-1,v+2)

%% this can lead to non-termination if v is considered static.  

%% Is this a problem for us? 



