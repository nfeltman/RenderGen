\section{Related Work}
\label{sec:related}

As Jorring and Scherlis argue frequency reduction and precomputation
are common techniques for both designing algorithms and performing
compiler optimizations~\cite{JS86-staging}.  The idea behind frequency
reduction is to identify computations that are performed multiple
times and pull them ahead so that they can be performed once and used
later as needed.  The idea behind precomputation is to identify
computations that can be performed earlier, for example at compile
time if their inputs are available statically and perform them at that
earlier time.  Many interesting algorithm design techniques and
compiler optimizations perform frequency reduction and precomputation.
For example, dynamic programming and loop hoisting are a frequency
reduction techniques, whereas dynamic data structures and incremental
computation are precomputation techniques.

Perhaps one of the most well studied example of precomputation is
partial evaluation.  Partial evaluation distinguishes between a static
(compile-time) and dynamic (run-time) stage. Given a program and the
values of its static inputs, which are the subset of the inputs that
become available at compilation time, partial evaluation generates a
specialized program by performing the computations that depend on the
statics inputs~\cite{jones96}.  The roots of partial evaluation go
back to Kleene, who in 1952 proved that a multivariate program
(function) $f(x_1, \ldots, x_{n+m})$ can be specialized for any given
$a_1 \ldots a_m$ to obtain another program $g(x_1, \ldots, x_n)$ such
that $f(a_1, \ldots, a_m, y_1, \ldots, y_n) = g(y_1, \ldots,
y_n)$~\cite{Kleene52}.  Kleene did not intend to improve efficiency,
however.  Lombardi appears to be the first person to use the term in
his work on incremental computation~\cite{Lombardi67}.  Since then,
there has a lot of work on partial evaluation, only broad-strokes of
which we can mention in this here.  We refer the interested reader to
the excellent book by Jones, Gommard, and Sestoft for a comprehesive
discussion of work until early 90's~\cite{JGS93}.

At a high level, earlier approaches to partial evaluation can be
viewed as operating in two stages: binding time analysis and program
specialization.  For a multivariate program with clearly marked static
and dynamic arguments, binding-time analysis identifies all the parts
of the program that can be computed by the knowledge of static
arguments. Using this information and the static arguments themselves,
program specialization specializes the program to a partially
evaluated program that can be used with many different dynamic
arguments.  This approach has been applied to construct partial
evaluators for a number of languages such as
Scheme~\cite{OB91-Similix,Consel88-Schism}, considering for the most
part first-order subsets.

Experience with binding time analysis showed that it can be difficult
to control, leading to programs whose performance can be difficult to
predict. This led to investigations based on type systems for making
explicit in the program the stage of each
computation~\cite{GJ91-lambda,NN92-twolevel} and writing ``meta
programs'' that when evaluated at a stage, give another program to be
evaluated at the next stage. Jorring and Scherlis describe this idea
meta programming in their paper (~\cite{JS86-staging}) as one of the
three possible approaches to taking advantage of
staging. Davies~\cite{davies96} presented a logical construction of
binding-time type systems by deriving a type system via the
Curry-Howard isomorphism applied to temporal logic.  Davies and
Pfenning proposed a new type system for staged computation based on a
particular fragment of modal logic~\cite{DP01-modal}. The work on
MetaML extended type-based techniques to a full-scale language by
developing statically typed programming language based on ML that
enables the programmer to express programs with multiple
stages~\cite{Taha97,taha-thesis-99}.  MetaML's type system is similar
to Davies~\cite{davies96} but extends it in several important ways.
Nanevksi and Pfenning further extended the these techniques by
allowing free variable to occur within staged
computations~\cite{NP05-nn}.

The type-system component of our work is closely related to this later
line of work on metaprogramming and staged computation.  The specific
extension to the typed lambda calculus that we use here is based on
the circle-modality of Davies~\cite{DP01-modal}.  Our types differ in
small ways to account for the specific evaluation model that we
propose.  The key difference between our work and this prior work is
the staging or the splitting algorithm, which given a two-staged
program splits it into two stages that can be executed sequentially,
passing information from the earlier to the latter stage by
construction. 
%
\ur{Do prior work evaluate first-stage expressions inside second stage
expressions?}
%
Unlike in partial evaluation, our splitting algorithm requires no
knowledge of the arguments themselves. The only information that is
required is their staging. Using just the staging information, the
splitting algorithm can generate programs that can perform non-trivial
transformations on the code automatically.  As our examples illustrate
such staging transformations can improve the efficiency of the source
program, sometimes in asymptotically significant ways.  
We are not aware of prior work in the context of higher-order
functional languages that presents such a splitting algorithm.

Perhaps the closest work to ours is algorithm for ``data
specialization'' given by Knoblock and Ruf~\cite{knoblock96}.  As our
work, Knoblock and Ruf are concerned about the problem of splitting an
explicitly staged program into two stages.  However, they only
consider, simple first-order language and straight-line
programs. Automatic pass seperation has also been a topic of interest
in programming systems for computer graphics, where, to achieve
high-performance, programs are typically manually separated into
components by frequency of execution (Seperation into a pre-determined
set of per-frequency modules is \emph{required} by the execution model
of modern graphics architectures.)  The software engineering
challenges modifying multiple per-stage programs have led to
suggestions of writing graphics programs in an explicitly staged
programming language\,\cite{Proudfoot:2001,Foley:2011,He:2014}, and
deferring the task of pass seperation to the compiler. However, all
prior efforts, like that of Knock and Ruf\,\cite{knoblock96} have been
limited to simple, imperative languages.  

\paragraph{Memoization.} Leaves code structure unchanged.

\paragraph{Incremental computation.} Usually not applicable within a
computation, require ``mutator'' ``core'' behavior.


%% \section{Related Work}

%% \ur{
%% \begin{itemize}
%% \item 
%% I find it hard to distinguish between blue and black on printed paper.
%% \item
%% $\pause$ should be called ``promote'' or something like that. I
%%   believe this is what Taha calls cross stage persistence.  THat work should
%%   be cited.
%% \end{itemize}
%% }

%% \crem{\emph{This text came from section 3:}
%% \lang\ is adapted from \cite{davies96}, and is very similar to other staged
%% languages considered in the partial evaluation and metaprogramming literature.
%% Unlike \cite{davies96}, we restrict \lang\ to exactly two stages, to simplify
%% our presentation of our stage-splitting algorithm. We expect that the techniques
%% presented here extend to more stages.
%% Our dynamic semantics for \lang\ differ from those given in \cite{davies96} and
%% the metaprogramming literature, because we avoid duplicating stage \bbtwo\
%% computations. In addition to being more efficient, our non-duplicating semantics
%% are easier to compare with our splitting algorithm, whose development and
%% exposition is our main goal.
%% }

%% There are several distinctive aspect of our work which we wish to compare and contrast.
%% Firstly, \lang\ is a simply typed lambda calculus 
%% with explicit stage annotations and a non-duplicating semantics.
%% Its statics and dynamics enforce no backward information flow,
%% which a is necessary property to define a splitting transformation
%% that is equivalent to the dynamics.
%% Finally, the splitting transformation itself operates on any well-typed open term in \lang.

%% Partial evaluation is a well-studied technique for specializing programs to some of their input.  
%% Often, the goal of partial evaluation systems is to go all the way from unstaged code to a specialized version.
%% Although this problem definition implicitly includes binding time analysis,
%% many presentations factor out that step and start with a stage-annotated representation, as we have done.
%% We've shown that our dynamic semantics can be used to perform partial evaluation 
%% [need to do more reading to know exactly how it compares].

%% One unifying feature of partial evaluation systems is that they operate on code which is closed at the first stage,
%% and so they must necessarily unfold recursive calls and wait for all stage \bbone\ input to operate.
%% There has been some work to avoid these issues by templating [need citations].
%% Stage-splitting, in contrast, goes all the way to operating on terms which are open on first stage variables,
%% and so these problems are avoided.


%% Stage-splitting itself has appeared in the literature---under 
%% the names {\em pass separation} and {\em data specialization}--- for various simpler input languages.

%% Most often, splitting is considered a compiler optimization (\cite{jorring86},\cite{knoblock96})
%% to minimize recomputation.
%% In this use case, only the arguments to the program are annotated with stages, 
%% and the splitter is left to perform an {\em ad hoc} binding time analysis to determine the 
%% internal staging of the program.
%% Because this occurs without programmer supervision, the binding time analyzers for these uses are 
%% reluctant to place stage \bbone\ code within stage \bbtwo\ conditionals, 
%% since that would result in speculative behavior that could increase the overall runtime.
%% We don't worry about this since the annotations of \lang\ are assumed to represent the will of a programmer.

%% Like the previous example, the Spark language (\cite{sparkThesis}) uses staging to minimize recomputation in real-time rendering applications.  
%% But instead of using a binding-time analysis to determine what parts of the program to put in what shader, 
%% Spark gives the programmer manual control of term staging using a language that is in spirit similar to \lang. 

%% We are also not the first to use stage splitting to derive algorithms.
%% \cite{malmkjaer89}, expanding on the work of \cite{barzdins88}, defined a stage-splitter
%% for a language of tail-recursive first-order equations with explicit staging annotations [double check that],
%% and considered applications to string matching.

%% Our work is distinct from all previous stage splitters in the following ways:
%% \begin{itemize}
%% \item Our input language, \lang, has a disciplined type theory.
%% \item \lang\ has full first-class functions (and by extension, recursion), as well as sum types.
%% \item Our stage splitting is proven correct.
%% \end{itemize}

%% The goal of {\em metaprogramming} (\cite{taha-thesis-99}, \cite{devito13}, \cite{davies01}) is 
%% to use code in a host language to write code in an object language.
%% Like partial evaluation and stage-splitting, this can be modeled by a staged language with explicit annotations.
%% Instead of identifying stages with points in time, stages are identified with host/object levels of the language,
%% and so type safety ensures the proper separation of levels of the language and safety of the constructed object code.

%% But the differences in goals of metaprogramming do cause a few differences in form.
%% For example, it's generally preferred in the metaprogramming context to use 
%% a dynamics that duplicates the contents of $\next$ blocks.
%% Moreover, in metaprogramming systems it makes sense to define an operator, \texttt{run}, 
%% which evaluates stage \bbtwo\ code to get an answer back at stage \bbone.
%% This is essentially a form a backwards information flow.
%% Thus, the \texttt{run} feature is directly incompatible with a temporal interpretation of stages,
%% which stage-splitting requires.

%% \cite{davies96} explored the connection between linear temporal logic and its corresponding type system (which is nearly identical to that for \lang), 
%% and showed the equivalence between that type system and existing one for binding time analysis. 
%% That work also provided a metaprogramming-style duplicating dynamic semantics, which we purposely avoided.
%% That said, the dynamics of \cite{davies96} is otherwise very similar to ours.
%% In fact, if we change the $\next$ and $\prev$ rules of ours to 
%% \begin{mathpar}
%% \infer {\diaone {\next~e}{\cdot,\next~q}} {\diatwo e q} \and
%% \infer {\diatwo{\prev~e}{v}} {\diaone e {\cdot,\next~v}} 
%% \end{mathpar}
%% and treat $\next~q$ as a partial value for any residual $q$,
%% then our semantics becomes rule-for-rule isomorphic to that from \cite{davies96}. This essentially bypasses the
%% environment bookkeeping in $\redonesym$, by inlining residuals instead of
%% hoisting them in \verb|let|-bindings.
%% From this it's clear that the semantics of \cite{davies96} and ours always produce the same value when they both terminate.
%% {\em However because \lang's semantics dictate that a reified residual will always be evaluated, regardless of whether its result is consumed, \lang\ programs terminate strictly less often than that of \cite{davies96}}.












%% Online and offline.  Both approaches have their merits but online
%% partial evaluation can be too slow and if stopped lead to an
%% inefficient residual.  Offline partial evaluation is faster.  Also,
%% full self-application has only been achieved by offline partial
%% evaluation.


%% Weaknesses of partial evaluation include~\cite{GJ05}:
%% 1) speedups linear in the subject programs run time (excuse moi?)

%% 2) good speedups require close knowledge of the program and some
%% binding-time improvements.

%% 3) result of specialization can be hard to predict as it can lead to
%% slowdown, code explosion, and non-termination at specialization time.


%% \paragraph{Meta programming}

%% What happens when we try to write quickselect as a meta program? 


%% \ur{Umut: I am starting a paragraph on termination.  we might need to expand
%%   on this.}
%% \paragraph{Termination.}

%% There are two reasons for non-termination: infinite loops and
%% execution of errorneous code.  The splitting algorithm can cause the
%% first.  But can it also cause the second? (If all computations really
%% depend on stage 1 code, then would it be possible to for example see
%% something like 1/0?  i think so but only if there is a meta dependency
%% between the data in two stages?)


%% Is in possible for the residual program to not terminate (unexpectedly)? 

%% Termination analysis in partial evaluation: see paper~\cite{AH96,GJ05}.

%% Based on Glenstrup and Jones (pp. 1154, 1155), a partial evaluator
%% should be complete. That is every computation depending on the static
%% input should be reduced to a value (during specialization).
%% Unfortunately, completeness is difficult to achieve if we also require
%% that the specialization is always terminating.

%% One problem with their description is that it is not clear which
%% termination problem they are talking about termination of the residual
%% or the specializer. 

%% Nontermination during specialization: 
%% consider code

%% double x = doubleplus (x,0) 

%% doubleplus (u,v) = if u <= 0 then v else doubleplus (u-1,v+2)

%% this can lead to non-termination if v is considered static.  

%% Is this a problem for us? 



%% \paragraph{Supercompilation.}
%% ~\cite{Turchin86}


%% \paragraph{Intro.}

%% A predominating theme in the design, implementatation, and
%% optimization of algorithm and software is frequency reduction and
%% precomputation.  The idea is to reduce the frequency of computations
%% by reducing their frequency and ideally by precomputing when possible.


%% The reason for why this theme is as predominant as it is that it is
%% naturally common in many actual applications.  For example, we may
%% have a collection of names or numbers that we frequently lookup, as
%% for example part of a membership test as part of a larger system.  In
%% some applications areas such as in graphics (the motivation for our
%% work), this is so common that there is a relatively large literature
%% on speeding up graphics computations by taking advantage of frequency
%% reduction and precomputation.

%% One way to express frequency of computations is to allow the
%% programmer to stage computation explicitly by making explicit the
%% stage of each computation and then by moving computations between
%% stages to improve efficiency by performing work as early (to save time
%% later when performance might be more critical) and  as infrequently
%% as possible (to reduce overall time).






%% For example, in meta-programming~\cite{}, the programmer can assign a
%% program expression a stage (stage 1, 2, 3, ...), which indicates the
%% stage at which that expression evaluates.  Combined with the ability
%% for earlier stages to construct expressions of later stages,
%% meta-programming techniques can be used to construct efficient
%% programs that improve efficiency over their non-staged counterparts by
%% reducing frequency reduction and precomputation.  For example, the
%% programmer can write a program that takes a list of numbers $\ell$ as
%% a first stage value and returns a second-stage function that performs
%% a fast membership test on that list $\ell$ by inlining all the
%% comparisons.  When given a key to lookup in the second stage, the
%% membership test function would quickly return the result.




