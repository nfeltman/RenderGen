%!TEX root = paper.tex

\section{Related Work}
\label{sec:related}

Frequency reduction and precomputation are common techniques for both
designing algorithms and performing compiler
optimizations~\cite{JS86-staging}.
%
The idea behind precomputation is to identify computations that can be
performed earlier (e.g., at compile time) if their inputs are
available statically and perform them at that earlier time. Dynamic
algorithms, partial evaluation, and incremental computation are all
examples of precomputation techniques.
%
The idea behind frequency reduction is to identify computations that
are performed multiple times and pull them ahead so that they can be
performed once and used later as needed.  Dynamic programming, loop
hoisting, and splitting (presented here) are all examples of frequency
reduction techniques.

\paragraph{Precomputation techniques.}
Perhaps one of the most studied examples of precomputation is partial
evaluation, which distinguishes between static (compile-time) and
dynamic (runtime) stages. Given a program and values for all static
inputs, partial evaluation generates a specialized program by
performing computations that depend only on the static
inputs~\cite{jones96}.  The roots of partial evaluation go back to
Kleene's $s$-$m$-$n$ theorem~\cite{Kleene52},
%Kleene did not intend to improve efficiency, however.  Lombardi
%appears to be the first person to use the term in his work on
%incremental computation~\cite{Lombardi67}.
%There has since been an enormous body of work on partial evaluation,
and we refer the reader to the book by Jones, Gommard, and Sestoft for
a comprehensive discussion of partial evaluation work until the early
90's~\cite{JGS93}.

Early approaches to partial evaluation can be viewed as operating in
two stages: binding time analysis and program specialization.  For a
multivariate program with clearly marked static and dynamic arguments,
binding-time analysis identifies all the parts of the program that can
be computed by the knowledge of static arguments. Using this
information and the values of static arguments, program specialization
specializes the original program to a partially-evaluated one that
operates on many different dynamic arguments.  This approach has been
applied to construct partial evaluators for a number of languages such
as Scheme~\cite{OB91-Similix,Consel88-Schism}.

Researchers have explored other staging transformations that, 
like splitting, partition an input two-stage program into two components,
on corresponding to each stage.  
In particular, \emph{binding time separation}\,\cite{Mogensen89a} 
(also called \emph{program bifurcation}\,\cite{DeNiel91})
has been used as a preprocessor step in partial evaluators,
allowing efficient specialization of programs with 
mixed-stage data structures without changes to the specializer itself.
Notably, the grammar-based binding-time specifications used in binding 
time separation are capable of describing
data structures with purely-static, purely-dynamic, and mixed-stage
content, much like the type system of \lang\
(though this correspondence is less clear without our addition of 
\bbonep\ and $\curr$).

However unlike splitting, where the goal is to emit code where first stage results
are computed once and then reused in multiple invocations of second
stage execution, the second (dynamic) function produced by binding time separation
does not use the results of the first; instead,
%only serves as a preprocessing step for the subsequent
%specialization phase of partial evaluation.
it has access to the first stage inputs and recomputes all required
first stage computations. As noted by Knoblock and
Ruf\,\cite{knoblock96}, it may be possible to modify the program
bifurcation algorithm to cache and reuse the intermediate results, but
this was never attempted.

Experience with binding time analysis showed that it can be difficult
to control, leading to programs whose performance was difficult to
predict. This led to investigations based on type systems for making
the stage of computations explicit in
programs~\cite{GJ91-lambda,NN92-twolevel} and writing ``metaprograms''
that, when evaluated at a stage, yield a new program to be evaluated
at the next stage.  Davies~\cite{davies96} presented a logical
construction of binding-time type systems by deriving a type system
via the Curry-Howard correspondence applied to temporal logic.  Davies
and Pfenning proposed a new type system for staged computation based
on a particular fragment of modal logic~\cite{DP01-modal}. The work on
MetaML extended type-based techniques to a full-scale language by
developing a statically typed programming language based on ML that
enables the programmer to express programs with multiple
stages~\cite{Taha97,taha-thesis-99}.  MetaML's type system is similar
to Davies~\cite{davies96} but extends it in several important ways.
Nanevksi and Pfenning further extended the these techniques by
allowing free variables to occur within staged
computations~\cite{NP05-nn}.

The type-system of \lang\ is closely related to this later line of
work on metaprogramming and staged computation.  The specific
extension to the typed lambda calculus that we use here is based on
the $\fut$ modality of Davies~\cite{DP01-modal}.  Our types differ in
small ways to account for the specific evaluation model that we
propose; however, the key difference between our work and this prior
work is that we instead focus on the problem of splitting.

Another class of precomputation techniques is incremental computation,
where a program can efficiently respond to small changes in its inputs
by only recomputing parts of the computation affected by the
changes~\cite{DemersReTe81,PughTe89,RamalingamRe93,AcarBlBlHaTa09}.
Thus, incremental computation can be thought of as a staged model,
where changes in inputs come over a period of time in stages.
However, unlike splitting, incremental computation does not require
fixing any of its inputs and, in the general case, allows for all
program inputs to change. Thus, the benefits of incremental
computation depend on what changes to inputs are made. For example,
while it is possible to apply incremental computation to the
quickselect example in \ref{sec:overview}, techniques would unfold the
quickselect function based on the demanded ranks, potentially
incurring linear time cost at each step of the algorithm (as opposed
to the logarithmic result produced by splitting).  Unlike splitting,
incremental computation techniques must also maintain sophisticated
data structures dynamically at run time to track what computations
must be performed.

\paragraph{Stage Splitting.}

Algorithms for stage splitting have appeared in the literature under
the names {\em pass separation}\,\cite{JS86-staging} and {\em data
  specialization}\,\cite{knoblock96}.  Perhaps the closest work to
ours is the algorithm for data specialization given by Knoblock and
Ruf~\cite{knoblock96}, which also seeks to statically split an
explicitly staged program into two stages.  However, they only
consider a simple first-order language and straight-line
programs. Their work also treats all computations guarded by
second-stage conditionals as second-stage computations, which would
prevent optimization (via splitting) of programs such as quickselect.

As noted in \ref{sec:graphics}, splitting algorithms have also been a
topic of interest in programming systems for computer graphics, where,
to achieve high performance, programs are manually separated into
components by frequency of execution corresponding to graphics
hardware pipeline stages. The software engineering challenges of
modifying multiple per-stage programs have led to suggestions of
writing graphics programs in an explicitly-staged programming
language\,\cite{Proudfoot:2001,Foley:2011,He:2014} and deferring the
task of pass separation to the compiler. However, all prior splitting
efforts in computer graphics, like that of Knoblock and
Ruf\,\cite{knoblock96} have been limited to simple, imperative
languages.



